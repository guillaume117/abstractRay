{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv2d_to_matmul(input, conv_layer):\n",
    "    # Extract Conv2D parameters\n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    "    # Unfold (im2col) the input tensor\n",
    "    input_unf = F.unfold(input, kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride)\n",
    "\n",
    "    # Reshape the weight tensor of the conv layer\n",
    "    weight = conv_layer.weight.view(out_channels, -1)\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    output_unf = weight @ input_unf\n",
    "\n",
    "    # Reshape the output to the correct dimensions\n",
    "    output_height = (input.size(2) + 2*padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input.size(3) + 2*padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    output = output_unf.view(1, out_channels, output_height, output_width)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "input = torch.randn(1, 3, 5, 5)  # Batch size 1, 3 channels, 5x5 image\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Get the output using the conv2d_to_matmul function\n",
    "output = conv2d_to_matmul(input, conv_layer)\n",
    "\n",
    "# Verify against the standard convolution\n",
    "conv_output = conv_layer(input)\n",
    "\n",
    "print(\"Output from matrix multiplication:\")\n",
    "print(output)\n",
    "\n",
    "print(\"Output from standard convolution:\")\n",
    "print(conv_output)\n",
    "\n",
    "# Verify that the outputs are the same\n",
    "print(\"Difference between outputs:\", torch.abs(output - conv_output).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv2d_to_matmul(input, conv_layer):\n",
    "    # Extract Conv2D parameters\n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    "    # Unfold (im2col) the input tensor\n",
    "    input_unf = F.unfold(input, kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride)\n",
    "\n",
    "    # Reshape the weight tensor of the conv layer\n",
    "    weight = conv_layer.weight.view(out_channels, -1)\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    output_unf = weight @ input_unf\n",
    "\n",
    "    # Add the bias\n",
    "    if conv_layer.bias is not None:\n",
    "        output_unf += conv_layer.bias.unsqueeze(1)\n",
    "\n",
    "    # Reshape the output to the correct dimensions\n",
    "    output_height = (input.size(2) + 2*padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input.size(3) + 2*padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    output = output_unf.view(1, out_channels, output_height, output_width)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "input = torch.randn(1, 3, 5, 5)  # Batch size 1, 3 channels, 5x5 image\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Get the output using the conv2d_to_matmul function\n",
    "output = conv2d_to_matmul(input, conv_layer)\n",
    "\n",
    "# Verify against the standard convolution\n",
    "conv_output = conv_layer(input)\n",
    "\n",
    "print(\"Output from matrix multiplication:\")\n",
    "print(output)\n",
    "\n",
    "print(\"Output from standard convolution:\")\n",
    "print(conv_output)\n",
    "\n",
    "# Verify that the outputs are the same\n",
    "print(\"Difference between outputs:\", torch.abs(output - conv_output).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv2d_to_matmul(input, conv_layer):\n",
    "\n",
    "\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    " \n",
    "    input_unf = F.unfold(input, kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride)\n",
    "\n",
    "    weight = conv_layer.weight.view(out_channels, -1)\n",
    "\n",
    "\n",
    "    output_unf = weight @ input_unf\n",
    "\n",
    "    if conv_layer.bias is not None:\n",
    "        output_unf += conv_layer.bias.unsqueeze(1)\n",
    "\n",
    "    output_height = (input.size(2) + 2*padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input.size(3) + 2*padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    output = output_unf.view(1, out_channels, output_height, output_width)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    input = torch.randn(1, 3, 55, 55)  \n",
    "    conv_layer = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    output = conv2d_to_matmul(input, conv_layer)\n",
    "\n",
    "\n",
    "    conv_output = conv_layer(input)\n",
    "\n",
    "print(\"Output from matrix multiplication:\")\n",
    "print(output)\n",
    "\n",
    "print(\"Output from standard convolution:\")\n",
    "print(conv_output)\n",
    "\n",
    "\n",
    "print(\"Difference between outputs:\", torch.abs(output - conv_output).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "def spectral_norm(matrix):\n",
    "    u, s, v = torch.svd(matrix, some=True)\n",
    "    return s.max()\n",
    "\n",
    "def lipschitz_constant(network, input_shape):\n",
    "    lipschitz_constants = []\n",
    "\n",
    "    def register_hook(layer):\n",
    "        if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    weight = module.weight.data\n",
    "                elif isinstance(module, nn.Conv2d):\n",
    "                    weight = module.weight.data.view(module.out_channels, -1)\n",
    "                lipschitz_constants.append(spectral_norm(weight))\n",
    "            return layer.register_forward_hook(hook)\n",
    "\n",
    "    hooks = []\n",
    "    for layer in network.modules():\n",
    "        hook = register_hook(layer)\n",
    "        if hook:\n",
    "            hooks.append(hook)\n",
    "\n",
    "    input_tensor = torch.randn(*input_shape)\n",
    "    network(input_tensor)\n",
    "\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "\n",
    "    total_lipschitz_constant = torch.prod(torch.tensor(lipschitz_constants))\n",
    "\n",
    "    return total_lipschitz_constant.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg19(weights=models.VGG19_Weights.DEFAULT)\n",
    "input_shape = (1, 3, 224, 224\n",
    "               ) \n",
    "lipschitz_const = lipschitz_constant(model, input_shape)\n",
    "print(f\"Constante de Lipschitz du réseau: {lipschitz_const}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "def find_maximizing_input(network, target_class, input_shape, num_iterations=300, learning_rate=0.01):\n",
    " \n",
    "    input_tensor = torch.randn(input_shape, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([input_tensor], lr=learning_rate)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = network(input_tensor)\n",
    "\n",
    "        target_output = output[0, target_class]\n",
    "\n",
    "        loss = -target_output\n",
    "\n",
    "     \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_tensor.clamp_(0, 1)  \n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss.item()}\")\n",
    "\n",
    "    return input_tensor\n",
    "\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "target_class = 1\n",
    "\n",
    "maximizing_input = find_maximizing_input(vgg19, target_class, input_shape)\n",
    "\n",
    "print(\"Tenseur d'entrée qui maximise la sortie:\")\n",
    "print(maximizing_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "maximizing_input_np = maximizing_input_1_t.squeeze(0).detach().numpy()\n",
    "maximizing_input_np = np.transpose(maximizing_input_np, (1, 2, 0))\n",
    "\n",
    "# Afficher l'image qui maximise la sortie du réseau\n",
    "plt.imshow(maximizing_input_np)\n",
    "plt.title(\"Image qui maximise la projection sur les valeurs singulières du réseau\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "def find_maximizing_input(network, num_iterations=100, learning_rate=0.01,input_tensor = torch.randn(3,224,224),ortho = 1,svd_rank = 0):\n",
    "    # Initialiser une entrée aléatoire\n",
    "\n",
    "    #input_tensor = maximizing_input_2\n",
    "\n",
    "    # Définir l'optimiseur\n",
    "    optimizer = optim.Adam([input_tensor], lr=learning_rate)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculer la sortie du réseau\n",
    "        output = input_tensor\n",
    "        projections = []\n",
    "\n",
    "        for layer in network.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                weight = layer.weight.data.view(layer.out_channels, -1)\n",
    "                u, s, v = torch.svd(weight)\n",
    "                v1 = v[:, svd_rank] \n",
    "                output_unfolded = F.unfold(output, kernel_size=layer.kernel_size, padding=layer.padding, stride=layer.stride)\n",
    "                projection = torch.matmul(v1, output_unfolded)\n",
    "                projections.append(projection)\n",
    "                output = layer(output)\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                weight = layer.weight.data\n",
    "                u, s, v = torch.svd(weight)\n",
    "                v1 = v[:, svd_rank]  \n",
    "                projection = torch.matmul(v1, output.view(output.size(0), -1).t())\n",
    "                projections.append(projection)\n",
    "                output = layer(output.view(output.size(0), -1))\n",
    "            elif isinstance(layer, nn.Sequential) or isinstance(layer, nn.ReLU) or isinstance(layer, nn.MaxPool2d):\n",
    "                output = layer(output)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Layer type {type(layer)} is not supported.\")\n",
    "\n",
    "        total_projection = sum(torch.norm(p) for p in projections)\n",
    "\n",
    "      \n",
    "        loss = -ortho*total_projection\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "       \n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_tensor.clamp_(0, 1) \n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss.item()}\")\n",
    "\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "\n",
    "input_tensor_0 = torch.randn(3,224,224, requires_grad=True)\n",
    "#maximizing_input_0 = find_maximizing_input(vgg19.features, input_tensor = input_tensor, ortho = 1,svd_rank=0)\n",
    "#maximizing_input_0_t = find_maximizing_input(vgg19.features, input_tensor = input_tensor, ortho = -1,svd_rank=0)\n",
    "maximizing_input_1 = find_maximizing_input(vgg19.features, input_tensor = input_tensor_0, ortho = 1,svd_rank=1)\n",
    "maximizing_input_1_t = find_maximizing_input(vgg19.features, input_tensor = input_tensor_0, ortho = -1,svd_rank=1)\n",
    "maximizing_input_2 = find_maximizing_input(vgg19.features, input_tensor = input_tensor_0, ortho = 1,svd_rank=2)\n",
    "maximizing_input_2_t = find_maximizing_input(vgg19.features, input_tensor = input_tensor_0, ortho = -1,svd_rank=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(model(maximizing_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(maximizing_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "u = torch.randn(10000,3,52,52)\n",
    "x = torch.where(u>0.5,u,0).to_sparse()\n",
    "y = torch.where(u>0.5,u,0).to_sparse()\n",
    "\n",
    "(u[0]*x).to_dense().shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_chunk(x, available_RAM):\n",
    "        dense_memory_footprint = torch.prod(torch.tensor(x.shape)) *4/1e9\n",
    "        return max(1,available_RAM//(4*dense_memory_footprint))\n",
    "x = torch.randn(1000,1000,100\n",
    "            )\n",
    "dim_chunk(x,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_to_matmul(input, conv_layer):\n",
    " \n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    "\n",
    "    input_unf = F.unfold(input, kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride)\n",
    "\n",
    "\n",
    "    weight = conv_layer.weight.view(out_channels, -1)\n",
    "\n",
    "  \n",
    "    output_unf = weight @ input_unf\n",
    "\n",
    "\n",
    "    output_height = (input.size(2) + 2*padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input.size(3) + 2*padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    output = output_unf.view(1, out_channels, output_height, output_width)\n",
    "\n",
    "    return output\n",
    "\n",
    "input = torch.randn(1, 3, 32, 32)  \n",
    "conv_layer = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  #\n",
    "\n",
    "output = conv2d_to_matmul(input, conv_layer)\n",
    "print(output.shape)  #\n",
    "print(torch.max(output -conv_layer(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv2d_to_matmul(input, conv_layer):\n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    "    output_height = (input.size(2) + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input.size(3) + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "\n",
    "    weight = conv_layer.weight.view(out_channels, in_channels, -1)\n",
    "    weight = weight.permute(1, 2, 0).contiguous().view(-1, out_channels)\n",
    "\n",
    "    input_unf = F.unfold(input, kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride)\n",
    "    \n",
    "\n",
    "    output_unf = weight.t() @ input_unf\n",
    "\n",
    "    output = output_unf.view(1, out_channels, output_height, output_width)\n",
    "\n",
    "    return output\n",
    "\n",
    "input = torch.randn(1, 3, 32, 32)  \n",
    "conv_layer = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  #\n",
    "\n",
    "output = conv2d_to_matmul(input, conv_layer)\n",
    "print(output.shape)  #\n",
    "print(torch.max(output -conv_layer(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sparse_conv2d_to_matmul(input_sparse, conv_layer):\n",
    "    assert input_sparse.layout == torch.sparse_coo, \"L'entrée doit être un tenseur sparse.\"\n",
    "\n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    "    indices = input_sparse._indices()\n",
    "    values = input_sparse._values()\n",
    "\n",
    "    # Calculer la taille de la sortie\n",
    "    input_height, input_width = input_sparse.size(2), input_sparse.size(3)\n",
    "    output_height = (input_height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input_width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    \n",
    "    # Initialiser la sortie sparse\n",
    "    output = torch.zeros((1, out_channels, output_height, output_width), device=input_sparse.device)\n",
    "\n",
    "    # Extraire les poids du noyau\n",
    "    weight = conv_layer.weight.view(out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "\n",
    "    # Appliquer la convolution manuellement\n",
    "    for i in range(indices.size(1)):\n",
    "        b, c, h, w = indices[:, i]\n",
    "        if h >= padding[0] and h < input_height + padding[0] and w >= padding[1] and w < input_width + padding[1]:\n",
    "            for kh in range(kernel_size[0]):\n",
    "                for kw in range(kernel_size[1]):\n",
    "                    if (h - kh * dilation[0] + padding[0]) % stride[0] == 0 and (w - kw * dilation[1] + padding[1]) % stride[1] == 0:\n",
    "                        h_out = (h - kh * dilation[0] + padding[0]) // stride[0]\n",
    "                        w_out = (w - kw * dilation[1] + padding[1]) // stride[1]\n",
    "                        if h_out >= 0 and h_out < output_height and w_out >= 0 and w_out < output_width:\n",
    "                            output[0, :, h_out, w_out] += weight[:, c, kh, kw] * values[i]\n",
    "\n",
    "    return output\n",
    "\n",
    "# Exemple d'utilisation\n",
    "input_sparse = torch.sparse_coo_tensor(\n",
    "    indices=[[0, 0, 0, 1], [0, 1, 2, 2], [2, 3, 2, 2], [2, 2, 4, 2]],\n",
    "    values=[1.0, 2.0, 3.0, 4.0],\n",
    "    size=(10, 3, 32, 32)\n",
    ")  # Exemple de tenseur d'entrée sparse\n",
    "\n",
    "conv_layer = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Exemple de couche de convolution\n",
    "conv_layer.bias.data = torch.zeros_like(conv_layer.bias.data)\n",
    "\n",
    "output = sparse_conv2d_to_matmul(input_sparse, conv_layer)\n",
    "print(output.shape)  # Devrait correspondre à la sortie d'une couche de convolution\n",
    "output_2 = conv_layer(input_sparse.to_dense())\n",
    "print(torch.max(output-output_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d\n",
    "\n",
    "def construct_convolution_matrix(conv_layer, input_size):\n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    "    input_height, input_width = input_size\n",
    "    output_height = (input_height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input_width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "    \n",
    "    weight = conv_layer.weight.view(out_channels, in_channels * kernel_size[0] * kernel_size[1])\n",
    "\n",
    "    # Matrice de convolution C\n",
    "    C = torch.zeros(out_channels * output_height * output_width, in_channels * input_height * input_width)\n",
    "\n",
    "    for oh in range(output_height):\n",
    "        for ow in range(output_width):\n",
    "            for kh in range(kernel_size[0]):\n",
    "                for kw in range(kernel_size[1]):\n",
    "                    for ic in range(in_channels):\n",
    "                        ih = oh * stride[0] - padding[0] + kh * dilation[0]\n",
    "                        iw = ow * stride[1] - padding[1] + kw * dilation[1]\n",
    "                        if 0 <= ih < input_height and 0 <= iw < input_width:\n",
    "                            for oc in range(out_channels):\n",
    "                                C[oc * output_height * output_width + oh * output_width + ow, \n",
    "                                  ic * input_height * input_width + ih * input_width + iw] = weight[oc, ic * kernel_size[0] * kernel_size[1] + kh * kernel_size[1] + kw]\n",
    "\n",
    "    return C, output_height, output_width\n",
    "\n",
    "def conv2d_to_matmul(input, conv_layer):\n",
    "    input_size = input.size()[2:]\n",
    "\n",
    "    C, output_height, output_width = construct_convolution_matrix(conv_layer, input_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output_flat = C * input\n",
    "    \n",
    "    \n",
    "    return output\n",
    "\n",
    "# Exemple d'utilisation\n",
    "input = torch.randn(10, 3, 32, 32)\n",
    "input = torch.where(input>0.5,input, 0).to_sparse()  # Exemple d'entrée\n",
    "conv_layer = Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Exemple de couche de convolution\n",
    "\n",
    "output = conv2d_to_matmul(input, conv_layer)\n",
    "print(output.shape)  # Devrait correspondre à la sortie d'une couche de convolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d\n",
    "\n",
    "def construct_convolution_matrix(conv_layer, input_size):\n",
    "    in_channels = conv_layer.in_channels\n",
    "    out_channels = conv_layer.out_channels\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "\n",
    "    input_height, input_width = input_size\n",
    "    output_height = (input_height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n",
    "    output_width = (input_width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n",
    "\n",
    "    # Initialiser la matrice de convolution C\n",
    "    C = torch.zeros(out_channels * output_height * output_width, in_channels * input_height * input_width)\n",
    "\n",
    "    # Extraire les poids du noyau\n",
    "    weight = conv_layer.weight.view(out_channels, in_channels * kernel_size[0] * kernel_size[1])\n",
    "\n",
    "    # Remplir la matrice de convolution C\n",
    "    for oc in range(out_channels):\n",
    "        for ic in range(in_channels):\n",
    "            for kh in range(kernel_size[0]):\n",
    "                for kw in range(kernel_size[1]):\n",
    "                    for oh in range(output_height):\n",
    "                        for ow in range(output_width):\n",
    "                            ih = oh * stride[0] - padding[0] + kh * dilation[0]\n",
    "                            iw = ow * stride[1] - padding[1] + kw * dilation[1]\n",
    "                            if 0 <= ih < input_height and 0 <= iw < input_width:\n",
    "                                C[oc * output_height * output_width + oh * output_width + ow, \n",
    "                                  ic * input_height * input_width + ih * input_width + iw] = weight[oc, ic * kernel_size[0] * kernel_size[1] + kh * kernel_size[1] + kw]\n",
    "    return C, output_height, output_width\n",
    "\n",
    "def conv2d_to_matmul(input, conv_layer):\n",
    "    input_size = input.size()[2:]\n",
    "    C, output_height, output_width = construct_convolution_matrix(conv_layer, input_size)\n",
    "    \n",
    "    input_flat = input.view(-1)\n",
    "    \n",
    "    output_flat = C @ input_flat\n",
    "    \n",
    "    output = output_flat.view(1, conv_layer.out_channels, output_height, output_width)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Exemple d'utilisation\n",
    "input = torch.randn(10, 3, 32, 32)  # Exemple d'entrée\n",
    "conv_layer = Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Exemple de couche de convolution\n",
    "\n",
    "output = conv2d_to_matmul(input, conv_layer)\n",
    "print(output.shape)  # Devrait correspondre à la sortie d'une couche de convolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UnStackNetwork:\n",
    "    def __init__(self, model, input_dim):\n",
    "        self.model = model\n",
    "        self.input_dim = input_dim\n",
    "        self.output = {}\n",
    "        self.unstack_network()\n",
    "\n",
    "    def unstack_network(self):\n",
    "        x = torch.randn(1, *self.input_dim)\n",
    "        for name, module in self.model.named_children():\n",
    "            if isinstance(module, nn.Sequential):\n",
    "                for layer_name, layer in module.named_children():\n",
    "                    x = self.process_layer(f\"{name}_{layer_name}\", layer, x)\n",
    "            else:\n",
    "                x = self.process_layer(name, module, x)\n",
    "\n",
    "    def process_layer(self, name, layer, x):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Flatten the tensor before passing to fully connected layers\n",
    "            self.process_flatten(name, nn.Flatten(), x)\n",
    "            x = nn.Flatten()(x)\n",
    "            self.process_linear_layer(name, layer, x)\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            self.process_conv_layer(name, layer, x)\n",
    "        elif isinstance(layer, (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.AdaptiveAvgPool2d, nn.MaxPool2d)):\n",
    "            self.process_activation_layer(name, layer)\n",
    "        x = layer(x) if layer is not None else x  # Handle input layer without layer\n",
    "        return x\n",
    "\n",
    "    def process_linear_layer(self, name, layer, x):\n",
    "        self.output[name] = {\n",
    "            'type': type(layer),\n",
    "            'original': copy.deepcopy(layer),\n",
    "            'epsilon_{}'.format(name): self.copy_with_zero_bias(layer),\n",
    "            'noise_{}'.format(name): self.copy_with_abs_weights(layer),\n",
    "            'output_dim': self.compute_output_dim(layer, x)\n",
    "        }\n",
    "\n",
    "    def process_flatten(self, name, layer, x):\n",
    "        self.output[f'{name}_flatten'] = {\n",
    "            'type': type(layer),\n",
    "            'original': layer,\n",
    "            'epsilon_{}'.format(f'{name}_flatten'): layer,\n",
    "            'noise_{}'.format(f'{name}_flatten'): layer,\n",
    "            'output_dim': self.compute_output_dim(layer, x)\n",
    "        }\n",
    "\n",
    "    def process_conv_layer(self, name, layer, x):\n",
    "        self.output[name] = {\n",
    "            'type': type(layer),\n",
    "            'original': copy.deepcopy(layer),\n",
    "            'epsilon_{}'.format(name): self.copy_with_zero_bias(layer),\n",
    "            'noise_{}'.format(name): self.copy_with_abs_weights(layer),\n",
    "            'output_dim': self.compute_output_dim(layer, x)\n",
    "        }\n",
    "\n",
    "    def process_activation_layer(self, name, layer):\n",
    "        self.output[name] = {\n",
    "            'activation': layer.__class__.__name__\n",
    "        }\n",
    "\n",
    "    def copy_with_zero_bias(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "            with torch.no_grad():\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "        return new_layer\n",
    "\n",
    "    def copy_with_abs_weights(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "                new_layer.weight.abs_()\n",
    "        return new_layer\n",
    "\n",
    "    def compute_output_dim(self, layer, x):\n",
    "        with torch.no_grad():\n",
    "            out = layer(x) if layer is not None else x  \n",
    "        return out.shape\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    " \n",
    "        self.fc1 = nn.Linear(in_features=32768, out_features=128)  \n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)  \n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu1(self.conv1(x))\n",
    "\n",
    "        x = self.relu2(self.conv2(x))\n",
    " \n",
    "\n",
    "  \n",
    "        x = x.view(x.size(0), -1)\n",
    "    \n",
    "        x = self.relu3(self.fc1(x))\n",
    "      \n",
    "        x = self.relu4(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class SimpleCNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN_2, self).__init__()\n",
    "        # Deux couches de convolution suivies de ReLU\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Deux couches entièrement connectées suivies de ReLU\n",
    "        self.fc1 = nn.Linear(in_features=32768, out_features=128)  # Supposons que la taille de l'entrée est 32x32\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)  # Supposons 10 classes de sortie\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Appliquer la première couche de convolution et ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Appliquer la deuxième couche de convolution et ReLU\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Appliquer le pooling pour réduire la taille (2x2 pooling)\n",
    "     \n",
    "        # Aplatir le tenseur pour l'entrée dans la couche entièrement connectée\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Appliquer la première couche entièrement connectée et ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Appliquer la deuxième couche entièrement connectée\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Instancier et utiliser la classe\n",
    "input_tensor = torch.randn(1, 3, 32, 32)  # Exemple de tenseur d'entrée\n",
    "model = SimpleCNN_2()\n",
    "r1 = model(input_tensor)\n",
    "torch.save(model, 'model_complete.pth')\n",
    "del model\n",
    "model= torch.load('model_complete.pth')\n",
    "print(model)\n",
    "r2 = model(input_tensor)\n",
    "processor = UnStackNetwork(model, input_tensor.shape[1:])\n",
    "print(processor.output)  # Afficher les résultats pour vérifier\n",
    "print(r1-r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(f\"Layer: {name}, Type: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLogger(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ActivationLogger, self).__init__()\n",
    "        self.model = model\n",
    "        self.activations = []\n",
    "        self.layers = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, module in self.model.named_children():\n",
    "            x = module(x)\n",
    "            self.layers.append((name, module))\n",
    "            if isinstance(module, (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.AdaptiveAvgPool2d, nn.MaxPool2d)):\n",
    "                self.activations.append((name, module))\n",
    "            elif isinstance(module, nn.Sequential):\n",
    "                for layer_name, layer in module.named_children():\n",
    "                    x = layer(x)\n",
    "                    self.layers.append((f\"{name}_{layer_name}\", layer))\n",
    "                    if isinstance(layer, (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.AdaptiveAvgPool2d, nn.MaxPool2d)):\n",
    "                        self.activations.append((f\"{name}_{layer_name}\", layer))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=torch.load('model_complete.pth')\n",
    "\n",
    "# Wrap the model\n",
    "logger = ActivationLogger(model)\n",
    "\n",
    "# Forward pass to log activations\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "output = logger(input_tensor)\n",
    "\n",
    "# Retrieve and print the structure and activations\n",
    "print(\"Layers:\")\n",
    "for name, layer in logger.layers:\n",
    "    print(f\"{name}: {layer}\")\n",
    "\n",
    "print(\"\\nActivations:\")\n",
    "for name, activation in logger.activations:\n",
    "    print(f\"{name}: {activation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Supposons que vous ayez un fichier 'network.pth' contenant le modèle\n",
    "model = torch.load('model_complete.pth')\n",
    "\n",
    "# Vérifiez que le modèle est une instance de nn.Module\n",
    "assert isinstance(model, nn.Module), \"Le modèle chargé n'est pas une instance de nn.Module\"\n",
    "\n",
    "# Définissez une classe wrapper\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Ajoutez ici tout traitement que vous souhaitez avant d'appeler la méthode forward d'origine\n",
    "        print(\"Avant l'appel de forward\")\n",
    "        \n",
    "        # Appelez la méthode forward d'origine\n",
    "        output = self.model.forward(*args, **kwargs)\n",
    "        \n",
    "        # Ajoutez ici tout traitement que vous souhaitez après l'appel de la méthode forward d'origine\n",
    "        print(\"Après l'appel de forward\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Créez une instance du wrapper avec le modèle chargé\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "# Utilisez le modèle wrappé comme d'habitude\n",
    "input_tensor = torch.randn(1, 3, 32, 32)  # Exemple de tenseur d'entrée\n",
    "output = wrapped_model(input_tensor)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "# Analyse de la méthode forward\n",
    "forward_method = model.forward\n",
    "source_code = inspect.getsource(forward_method)\n",
    "\n",
    "print(\"Code source de la méthode forward :\")\n",
    "print(source_code)\n",
    "\n",
    "# Afficher des informations supplémentaires\n",
    "signature = inspect.signature(forward_method)\n",
    "print(\"\\nSignature de la méthode forward :\")\n",
    "print(signature)\n",
    "\n",
    "docstring = inspect.getdoc(forward_method)\n",
    "print(\"\\nDocstring de la méthode forward :\")\n",
    "print(docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "\n",
    "\n",
    "class UnStackNetwork:\n",
    "    def __init__(self, model, input_dim):\n",
    "        self.model = model\n",
    "        self.input_dim = input_dim\n",
    "        self.output = {}\n",
    "        self.unstack_network()\n",
    "\n",
    "    def unstack_network(self):\n",
    "        x = torch.randn(1, *self.input_dim)\n",
    "        for name, module in self.model.named_children():\n",
    "            if isinstance(module, nn.Sequential):\n",
    "                for layer_name, layer in module.named_children():\n",
    "                    x = self.process_layer(f\"{name}_{layer_name}\", layer, x)\n",
    "            else:\n",
    "                x = self.process_layer(name, module, x)\n",
    "\n",
    "    def process_layer(self, name, layer, x):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Flatten the tensor before passing to fully connected layers\n",
    "            self.process_flatten(name, nn.Flatten(), x)\n",
    "            x = nn.Flatten()(x)\n",
    "            self.process_linear_layer(name, layer, x)\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            self.process_conv_layer(name, layer, x)\n",
    "        elif isinstance(layer, (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.AdaptiveAvgPool2d, nn.MaxPool2d, nn.BatchNorm2d, nn.Dropout)):\n",
    "            self.process_activation_layer(name, layer, x)\n",
    "        x = layer(x) if layer is not None else x  # Handle input layer without layer\n",
    "        return x\n",
    "\n",
    "    def process_linear_layer(self, name, layer, x):\n",
    "        self.output[name] = {\n",
    "            'type': type(layer),\n",
    "            'original': copy.deepcopy(layer),\n",
    "            'epsilon_{}'.format(name): self.copy_with_zero_bias(layer),\n",
    "            'noise_{}'.format(name): self.copy_with_abs_weights(layer),\n",
    "            'output_dim': self.compute_output_dim(layer, x)\n",
    "        }\n",
    "\n",
    "    def process_flatten(self, name, layer, x):\n",
    "        self.output[f'{name}_flatten'] = {\n",
    "            'type': type(layer),\n",
    "            'original': layer,\n",
    "            'epsilon_{}'.format(f'{name}_flatten'): layer,\n",
    "            'noise_{}'.format(f'{name}_flatten'): layer,\n",
    "            'output_dim': self.compute_output_dim(layer, x)\n",
    "        }\n",
    "\n",
    "    def process_conv_layer(self, name, layer, x):\n",
    "        self.output[name] = {\n",
    "            'type': type(layer),\n",
    "            'original': copy.deepcopy(layer),\n",
    "            'epsilon_{}'.format(name): self.copy_with_zero_bias(layer),\n",
    "            'noise_{}'.format(name): self.copy_with_abs_weights(layer),\n",
    "            'output_dim': self.compute_output_dim(layer, x)\n",
    "        }\n",
    "\n",
    "    def process_activation_layer(self, name, layer, x):\n",
    "        self.output[name] = {\n",
    "            'activation': layer.__class__.__name__,\n",
    "            'output_dim': self.compute_output_dim(layer, x)\n",
    "        }\n",
    "\n",
    "    def copy_with_zero_bias(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "            with torch.no_grad():\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "        return new_layer\n",
    "\n",
    "    def copy_with_abs_weights(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "                new_layer.weight.abs_()\n",
    "        return new_layer\n",
    "\n",
    "    def compute_output_dim(self, layer, x):\n",
    "        with torch.no_grad():\n",
    "            out = layer(x) if layer is not None else x\n",
    "        return out.shape\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "model = torch.load('model_complete.pth')\n",
    "input_dim = (3, 32, 32)  # Dimensions de l'entrée pour ResNet18\n",
    "unstacked_network = UnStackNetwork(model, input_dim)\n",
    "\n",
    "# Afficher la sortie\n",
    "for name, details in unstacked_network.output.items():\n",
    "    print(f\"{name}: {details}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_method = model.forward\n",
    "source_code = inspect.getsource(forward_method)\n",
    "print(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "class UnStackNetwork:\n",
    "    def __init__(self, model, input_dim):\n",
    "        self.model = model\n",
    "        self.input_dim = input_dim\n",
    "        self.output = {}\n",
    "        self.handles = []\n",
    "        self.register_hooks()\n",
    "        self.unstack_network()\n",
    "        self.remove_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def hook(module, input, output, name):\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                self.output[name] = {\n",
    "                    'type': type(module),\n",
    "                    'original': copy.deepcopy(module),\n",
    "                    'epsilon_{}'.format(name): self.copy_with_zero_bias(module),\n",
    "                    'noise_{}'.format(name): self.copy_with_abs_weights(module),\n",
    "                    'output_dim': output.shape\n",
    "                }\n",
    "            elif isinstance(module, nn.Flatten):\n",
    "                self.output[name] = {\n",
    "                    'type': type(module),\n",
    "                    'original': module,\n",
    "                    'epsilon_{}'.format(name): module,\n",
    "                    'noise_{}'.format(name): module,\n",
    "                    'output_dim': output.shape\n",
    "                }\n",
    "            elif isinstance(module, (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.AdaptiveAvgPool2d, nn.MaxPool2d, nn.BatchNorm2d, nn.Dropout)):\n",
    "                self.output[name] = {\n",
    "                    'activation': module.__class__.__name__,\n",
    "                    'output_dim': output.shape\n",
    "                }\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            handle = module.register_forward_hook(lambda module, input, output, name=name: hook(module, input, output, name))\n",
    "            self.handles.append(handle)\n",
    "\n",
    "    def unstack_network(self):\n",
    "        x = torch.randn(1, *self.input_dim)\n",
    "        self.model(x)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def copy_with_zero_bias(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "            with torch.no_grad():\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "        return new_layer\n",
    "\n",
    "    def copy_with_abs_weights(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "                new_layer.weight.abs_()\n",
    "        return new_layer\n",
    "\n",
    "# Exemple d'utilisation\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CustomModel()\n",
    "input_dim = (3, 32, 32)\n",
    "unstacked_network = UnStackNetwork(model, input_dim)\n",
    "\n",
    "# Afficher la sortie\n",
    "for name, details in unstacked_network.output.items():\n",
    "    print(f\"{name}: {details}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "class UnStackNetwork:\n",
    "    def __init__(self, model, input_dim):\n",
    "        self.model = model\n",
    "        self.input_dim = input_dim\n",
    "        self.output = {}\n",
    "        self.handles = []\n",
    "        self.layers = []\n",
    "        self.register_hooks()\n",
    "        self.unstack_network()\n",
    "        self.remove_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def hook(module, input, output, name):\n",
    "            self.layers.append((name, module, output))\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear, nn.Flatten)):\n",
    "                self.output[name] = {\n",
    "                    'type': type(module),\n",
    "                    'original': copy.deepcopy(module),\n",
    "                    'epsilon_{}'.format(name): self.copy_with_zero_bias(module),\n",
    "                    'noise_{}'.format(name): self.copy_with_abs_weights(module),\n",
    "                    'output_dim': output.shape\n",
    "                }\n",
    "            elif isinstance(module, (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.AdaptiveAvgPool2d, nn.MaxPool2d, nn.BatchNorm2d, nn.Dropout)):\n",
    "                self.output[name] = {\n",
    "                    'activation': module.__class__.__name__,\n",
    "                    'output_dim': output.shape\n",
    "                }\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            if not isinstance(module, nn.Sequential):  # Ignorer les séquentiels pour éviter les doublons\n",
    "                handle = module.register_forward_hook(lambda module, input, output, name=name: hook(module, input, output, name))\n",
    "                self.handles.append(handle)\n",
    "\n",
    "    def unstack_network(self):\n",
    "        x = torch.randn(1, *self.input_dim)\n",
    "        self.model(x)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def copy_with_zero_bias(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "            with torch.no_grad():\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "        return new_layer\n",
    "\n",
    "    def copy_with_abs_weights(self, layer):\n",
    "        new_layer = copy.deepcopy(layer)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
    "                if new_layer.bias is not None:\n",
    "                    new_layer.bias.zero_()\n",
    "                new_layer.weight.abs_()\n",
    "        return new_layer\n",
    "\n",
    "# Exemple d'utilisation\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CustomModel()\n",
    "input_dim = (3, 32, 32)\n",
    "unstacked_network = UnStackNetwork(model, input_dim)\n",
    "\n",
    "# Afficher la sortie\n",
    "for name, details in unstacked_network.output.items():\n",
    "    print(f\"{name}: {details}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx2torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from onnx2torch import convert\n",
    "path = './vgg16-12.onnx'\n",
    "onnx_model = onnx.load(path)\n",
    "target_version = 7\n",
    "pytorch_model = convert(onnx_model,target_version)\n",
    "pytorch_model = pytorch_model.eval()\n",
    "\n",
    "pytorch_model(torch.randn(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "forward_method = pytorch_model.forward\n",
    "source_code = inspect.getsource(forward_method)\n",
    "print(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def im2col(input, kernel_size, stride=1, padding=0):\n",
    "    # Apply padding\n",
    "    input_padded = F.pad(input, (padding, padding, padding, padding))\n",
    "    \n",
    "    # Extract the dimensions\n",
    "    batch_size, channels, height, width = input_padded.size()\n",
    "    out_height = (height - kernel_size) // stride + 1\n",
    "    out_width = (width - kernel_size) // stride + 1\n",
    "\n",
    "    col = torch.zeros(batch_size, channels, kernel_size, kernel_size, out_height, out_width)\n",
    "    for y in range(kernel_size):\n",
    "        y_max = y + stride * out_height\n",
    "        for x in range(kernel_size):\n",
    "            x_max = x + stride * out_width\n",
    "            col[:, :, y, x, :, :] = input_padded[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.permute(0, 4, 5, 1, 2, 3).contiguous()\n",
    "    col = col.view(batch_size, out_height * out_width, -1)\n",
    "    return col\n",
    "\n",
    "class Conv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "\n",
    "        self.out_channels, self.in_channels, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def conv_to_matmul(self, x):\n",
    "        batch_size, in_channels, height, width = x.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # im2col transformation\n",
    "        x_col = im2col(x, self.kernel_size, self.stride, self.padding)\n",
    "        x_col = x_col.view(batch_size, -1, self.kernel_size * self.kernel_size * self.in_channels)\n",
    "\n",
    "        # Reshape weight\n",
    "        weight_col = self.weight.view(self.out_channels, -1)\n",
    "\n",
    "        # Matrix multiplication\n",
    "        out = weight_col @ x_col.transpose(1, 2)\n",
    "        if self.bias is not None:\n",
    "            out += self.bias.view(1, -1, 1)\n",
    "\n",
    "        # Reshape back to the output shape\n",
    "        out = out.view(batch_size, self.out_channels, out_height, out_width)\n",
    "        return out\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 3\n",
    "    out_channels = 2\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul\n",
    "    conv_to_matmul = Conv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée\n",
    "    input_tensor = torch.randn(1, in_channels, 5, 5)\n",
    "\n",
    "    # Passage de l'entrée à travers la couche de convolution\n",
    "    conv_output = conv_layer(input_tensor)\n",
    "    print(\"Résultat de la convolution:\", conv_output)\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul\n",
    "    matmul_output = conv_to_matmul.conv_to_matmul(input_tensor)\n",
    "    print(\"Résultat du produit matriciel:\", matmul_output)\n",
    "\n",
    "    # Vérification de l'égalité des résultats\n",
    "    assert torch.allclose(conv_output, matmul_output, atol=1e-6), \"Les résultats ne correspondent pas!\"\n",
    "    print(\"Les résultats correspondent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "\n",
    "        self.out_channels, self.in_channels, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, in_channels, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        # Apply padding to the sparse tensor\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        # Prepare the output sparse tensor\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over the sparse values\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            for oc in range(self.out_channels):\n",
    "                conv_value = 0.0\n",
    "                for ic in range(self.in_channels):\n",
    "                    for ky in range(self.kernel_size):\n",
    "                        for kx in range(self.kernel_size):\n",
    "                            yy = y + ky - self.kernel_size // 2\n",
    "                            xx = x + kx - self.kernel_size // 2\n",
    "                            if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                                weight = self.weight[oc, ic, ky, kx].item()\n",
    "                                if weight != 0:\n",
    "                                    input_value = x_sparse[0, ic, yy, xx].item()\n",
    "                                    conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                output_indices.append([b.item(), oc, (y - self.kernel_size // 2) // self.stride, (x - self.kernel_size // 2) // self.stride])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 3\n",
    "    out_channels = 2\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(1, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "\n",
    "        self.out_channels, self.in_channels, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, in_channels, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        # Apply padding to the sparse tensor\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        # Prepare the output sparse tensor\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over the sparse values\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            for oc in range(self.out_channels):\n",
    "                conv_value = 0.0\n",
    "                for ic in range(self.in_channels):\n",
    "                    for ky in range(self.kernel_size):\n",
    "                        for kx in range(self.kernel_size):\n",
    "                            yy = y + ky - self.kernel_size // 2\n",
    "                            xx = x + kx - self.kernel_size // 2\n",
    "                            if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                                weight = self.weight[oc, ic, ky, kx].item()\n",
    "                                input_value = x_sparse[0, ic, yy, xx].item() if (0 <= yy < height and 0 <= xx < width) else 0\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                output_indices.append([b.item(), oc, (y - self.kernel_size // 2) // self.stride, (x - self.kernel_size // 2) // self.stride])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 2\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(1, in_channels, 5,5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "    conv_output =conv_layer(input_tensor_dense)\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense()-conv_output)\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "\n",
    "        self.out_channels, self.in_channels, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, in_channels, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        # Apply padding to the sparse tensor\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        # Prepare the output sparse tensor\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over the sparse values\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            for oc in range(self.out_channels):\n",
    "                conv_value = 0.0\n",
    "                for ic in range(self.in_channels):\n",
    "                    for ky in range(self.kernel_size):\n",
    "                        for kx in range(self.kernel_size):\n",
    "                            yy = y + ky - self.kernel_size // 2\n",
    "                            xx = x + kx - self.kernel_size // 2\n",
    "                            if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                                weight = self.weight[oc, ic, ky, kx].item()\n",
    "                                input_value = values[i] if (yy == y and xx == x and ic == c) else 0\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 2\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(1, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "    # Passage de l'entrée à travers la couche de convolution\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    # Vérification de l'égalité des résultats\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "\n",
    "        self.out_channels, self.in_channels, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, in_channels, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        # Apply padding to the sparse tensor\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        # Prepare the output sparse tensor\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over the sparse values\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            for oc in range(self.out_channels):\n",
    "                conv_value = 0.0\n",
    "                for ic in range(self.in_channels):\n",
    "                    for ky in range(self.kernel_size):\n",
    "                        for kx in range(self.kernel_size):\n",
    "                            yy = y + ky - self.kernel_size // 2\n",
    "                            xx = x + kx - self.kernel_size // 2\n",
    "                            if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                                weight = self.weight[oc, ic, ky, kx].item()\n",
    "                                input_value = values[(b == indices[0]) & (ic == indices[1]) & (yy == indices[2]) & (xx == indices[3])].sum().item()\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 2\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(1, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "    # Passage de l'entrée à travers la couche de convolution\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    # Vérification de l'égalité des résultats\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "\n",
    "        self.out_channels, self.in_channels, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_im2col(self, indices, values, height, width, kernel_size, stride=1, padding=0):\n",
    "        # Apply padding to the indices\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += padding\n",
    "\n",
    "        # Calculate output dimensions\n",
    "        out_height = (height + 2 * padding - kernel_size) // stride + 1\n",
    "        out_width = (width + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "        col_indices = []\n",
    "        col_values = []\n",
    "\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < kernel_size // 2 or y >= height + padding + kernel_size // 2:\n",
    "                continue\n",
    "            if x < kernel_size // 2 or x >= width + padding + kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            for ky in range(kernel_size):\n",
    "                for kx in range(kernel_size):\n",
    "                    yy = y + ky - kernel_size // 2\n",
    "                    xx = x + kx - kernel_size // 2\n",
    "                    if 0 <= yy < height + 2 * padding and 0 <= xx < width + 2 * padding:\n",
    "                        col_indices.append([b.item(), c.item(), ky, kx, yy // stride, xx // stride])\n",
    "                        col_values.append(values[i].item())\n",
    "\n",
    "        col_indices = torch.tensor(col_indices).t()\n",
    "        col_values = torch.tensor(col_values)\n",
    "\n",
    "        return col_indices, col_values\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, in_channels, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "\n",
    "        # Perform sparse im2col\n",
    "        col_indices, col_values = self.sparse_im2col(indices, values, height, width, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "        # Convert im2col result to sparse tensor\n",
    "        col_shape = (batch_size, in_channels, self.kernel_size, self.kernel_size, out_height, out_width)\n",
    "        col_sparse = torch.sparse.FloatTensor(col_indices, col_values, col_shape)\n",
    "\n",
    "        # Reshape weight\n",
    "        weight_col = self.weight.view(self.out_channels, -1)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        out_sparse = torch.sparse.mm(weight_col, col_sparse.view(in_channels * self.kernel_size * self.kernel_size, -1))\n",
    "\n",
    "        # Reshape output to the correct shape\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_indices = out_sparse._indices().t().view(-1, out_height, out_width).contiguous().view(-1, 3)\n",
    "        output_values = out_sparse._values()\n",
    "\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 2\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(1, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "    # Passage de l'entrée à travers la couche de convolution\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    # Vérification de l'égalité des résultats\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "\n",
    "        self.out_channels, self.in_channels, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, in_channels, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        # Apply padding to the sparse tensor\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        # Prepare the output sparse tensor\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over the sparse values\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            for oc in range(self.out_channels):\n",
    "                conv_value = 0.0\n",
    "                for ic in range(self.in_channels):\n",
    "                    for ky in range(self.kernel_size):\n",
    "                        for kx in range(self.kernel_size):\n",
    "                            yy = y + ky - self.kernel_size // 2\n",
    "                            xx = x + kx - self.kernel_size // 2\n",
    "                            if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                                weight = self.weight[oc, ic, ky, kx].item()\n",
    "                                mask = (padded_indices[0] == b) & (padded_indices[1] == ic) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                                if mask.any():\n",
    "                                    input_value = values[mask].item()\n",
    "                                    conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 1\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(10, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "    # Passage de l'entrée à travers la couche de convolution\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    # Vérification de l'égalité des résultats\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "        self.groups = conv_layer.groups\n",
    "\n",
    "        self.out_channels, self.in_channels_per_group, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.in_channels = self.in_channels_per_group * self.groups\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, _, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        # Apply padding to the sparse tensor\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        # Prepare the output sparse tensor\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over the sparse values\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            group = c // self.in_channels_per_group\n",
    "            oc_start = group * (self.out_channels // self.groups)\n",
    "            oc_end = oc_start + (self.out_channels // self.groups)\n",
    "            \n",
    "            for oc in range(oc_start, oc_end):\n",
    "                conv_value = 0.0\n",
    "                ic_in_group = c % self.in_channels_per_group\n",
    "                for ky in range(self.kernel_size):\n",
    "                    for kx in range(self.kernel_size):\n",
    "                        yy = y + ky - self.kernel_size // 2\n",
    "                        xx = x + kx - self.kernel_size // 2\n",
    "                        if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                            weight = self.weight[oc, ic_in_group, ky, kx].item()\n",
    "                            mask = (padded_indices[0] == b) & (padded_indices[1] == c) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                            if mask.any():\n",
    "                                input_value = values[mask].item()\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 6\n",
    "    out_channels = 12\n",
    "    kernel_size = 1\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    groups = 2\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(1, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "    # Passage de l'entrée à travers la couche de convolution\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    # Vérification de l'égalité des résultats\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "        self.groups = conv_layer.groups\n",
    "\n",
    "        self.out_channels, self.in_channels_per_group, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.in_channels = self.in_channels_per_group * self.groups\n",
    "        self.kernel_size = self.kernel_h  # assuming square kernels\n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, _, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Extract sparse indices and values\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        # Apply padding to the sparse tensor\n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        # Prepare the output sparse tensor\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over the sparse values\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            group = c // self.in_channels_per_group\n",
    "            oc_start = group * (self.out_channels // self.groups)\n",
    "            oc_end = oc_start + (self.out_channels // self.groups)\n",
    "            \n",
    "            for oc in range(oc_start, oc_end):\n",
    "                conv_value = 0.0\n",
    "                ic_in_group = c % self.in_channels_per_group\n",
    "                for ky in range(self.kernel_size):\n",
    "                    for kx in range(self.kernel_size):\n",
    "                        yy = y + ky - self.kernel_size // 2\n",
    "                        xx = x + kx - self.kernel_size // 2\n",
    "                        if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                            weight = self.weight[oc, ic_in_group, ky, kx].item()\n",
    "                            mask = (padded_indices[0] == b) & (padded_indices[1] == c) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                            if mask.any():\n",
    "                                input_value = values[mask].sum().item()\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Paramètres de la couche de convolution\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    groups = 3\n",
    "\n",
    "    # Création de la couche de convolution\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups)\n",
    "\n",
    "    # Création de l'instance de la transformation conv -> matmul pour tenseur sparse\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    # Exemple de tenseur d'entrée sparse\n",
    "    input_tensor_dense = torch.randn(1, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    # Passage de l'entrée à travers la transformation matmul pour tenseur sparse\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "    # Passage de l'entrée à travers la couche de convolution\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    # Vérification de l'égalité des résultats\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "\n",
    "    # Affichage du résultat\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight – filters of shape (out_channels,in_channelsgroups,kH,kW)(out_channels,groupsin_channels​,kH,kW)\n",
    "weight – filters of shape (out_channels,in_channelsgroups,kH,kW)(out_channels,groupsin_channels​,kH,kW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "        self.groups = conv_layer.groups\n",
    "        print(self.groups)\n",
    "\n",
    "        self.out_channels, self.in_channels_per_group, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.in_channels = self.in_channels_per_group * self.groups\n",
    "        self.kernel_size = self.kernel_h  \n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, _, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "     \n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "      \n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "  \n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            group = c // (self.in_channels // self.groups)\n",
    "            oc_start = group * (self.out_channels // self.groups)\n",
    "            oc_end = oc_start + (self.out_channels // self.groups)\n",
    "            \n",
    "            for oc in range(oc_start, oc_end):\n",
    "                conv_value = 0.0\n",
    "                ic_in_group = c % (self.in_channels // self.groups)\n",
    "                for ky in range(self.kernel_size):\n",
    "                    for kx in range(self.kernel_size):\n",
    "                        yy = y + ky - self.kernel_size // 2\n",
    "                        xx = x + kx - self.kernel_size // 2\n",
    "                        if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                            weight = self.weight[oc, ic_in_group, ky, kx].item()\n",
    "                            mask = (padded_indices[0] == b) & (padded_indices[1] == c) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                            if mask.any():\n",
    "                                input_value = values[mask].sum().item()\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices).t()\n",
    "        output_values = torch.tensor(output_values)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    groups = 3\n",
    "\n",
    "   \n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups)\n",
    "    conv_layer.bias.data=torch.zeros_like(conv_layer.bias.data)\n",
    "    \n",
    "\n",
    "  \n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    input_tensor_dense = torch.randn(10, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "    \n",
    "\n",
    "\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "\n",
    "\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('app/src')\n",
    "sys.path.append('./src')\n",
    "from zono_sparse_gen import ZonoSparseGeneration\n",
    "test_input = torch.randn(3,224,224)\n",
    "_,zonotope_espilon_sparse_tensor = ZonoSparseGeneration(test_input,0.01).total_zono()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('app/src')\n",
    "sys.path.append('./src')\n",
    "from zono_sparse_gen import ZonoSparseGeneration\n",
    "test_input = torch.randn(3,224,224)\n",
    "_,zonotope_espilon_sparse_tensor = ZonoSparseGeneration(test_input,0.01).total_zono()\n",
    "\n",
    "zonotope_espilon_sparse_tensor\n",
    "with torch.no_grad():\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(zonotope_espilon_sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_output_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "        self.groups = conv_layer.groups\n",
    "\n",
    "        self.out_channels, self.in_channels_per_group, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.in_channels = self.in_channels_per_group * self.groups\n",
    "        self.kernel_size = self.kernel_h  \n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, _, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            group = c // (self.in_channels // self.groups)\n",
    "            oc_start = group * (self.out_channels // self.groups)\n",
    "            oc_end = oc_start + (self.out_channels // self.groups)\n",
    "            \n",
    "            for oc in range(oc_start, oc_end):\n",
    "                conv_value = 0.0\n",
    "                ic_in_group = c % (self.in_channels // self.groups)\n",
    "                for ky in range(self.kernel_size):\n",
    "                    for kx in range(self.kernel_size):\n",
    "                        yy = y + ky - self.kernel_size // 2\n",
    "                        xx = x + kx - self.kernel_size // 2\n",
    "                        if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                            weight = self.weight[oc, ic_in_group, ky, kx].item()\n",
    "                            mask = (padded_indices[0] == b) & (padded_indices[1] == c) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                            if mask.any():\n",
    "                                input_value = values[mask].sum().item()\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices, dtype=torch.long).t()\n",
    "        output_values = torch.tensor(output_values, dtype=torch.float)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 2\n",
    "    padding = 1\n",
    "    groups = 1\n",
    "\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups)\n",
    "    conv_layer.bias.data = torch.zeros_like(conv_layer.bias.data)\n",
    "\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    input_tensor_dense = torch.randn(10, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "        self.groups = conv_layer.groups\n",
    "\n",
    "        self.out_channels, self.in_channels_per_group, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.in_channels = self.in_channels_per_group * self.groups\n",
    "        self.kernel_size = self.kernel_h  \n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, _, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            group = c // (self.in_channels // self.groups)\n",
    "            oc_start = group * (self.out_channels // self.groups)\n",
    "            oc_end = oc_start + (self.out_channels // self.groups)\n",
    "            \n",
    "            for oc in range(oc_start, oc_end):\n",
    "                conv_value = 0.0\n",
    "                ic_in_group = c % (self.in_channels // self.groups)\n",
    "                for ky in range(self.kernel_size):\n",
    "                    for kx in range(self.kernel_size):\n",
    "                        yy = y + ky - self.kernel_size // 2\n",
    "                        xx = x + kx - self.kernel_size // 2\n",
    "                        if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                            weight = self.weight[oc, ic_in_group, ky, kx].item()\n",
    "                            mask = (padded_indices[0] == b) & (padded_indices[1] == c) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                            if mask.any():\n",
    "                                input_value = values[mask].sum().item()\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                if out_y < 0 or out_y >= out_height or out_x < 0 or out_x >= out_width:\n",
    "                    continue\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices, dtype=torch.long).t()\n",
    "        output_values = torch.tensor(output_values, dtype=torch.float)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    groups = 1\n",
    "\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups)\n",
    "    conv_layer.bias.data = torch.zeros_like(conv_layer.bias.data)\n",
    "\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    input_tensor_dense = torch.randn(10, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "        self.groups = conv_layer.groups\n",
    "\n",
    "        self.out_channels, self.in_channels_per_group, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.in_channels = self.in_channels_per_group * self.groups\n",
    "        self.kernel_size = self.kernel_h  \n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, _, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        padded_indices = indices.clone()\n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over spatial indices first\n",
    "        for i in range(values.size(0)):\n",
    "            b, c, y, x = padded_indices[:, i]\n",
    "            if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                continue\n",
    "            if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                continue\n",
    "\n",
    "            group = c // (self.in_channels // self.groups)\n",
    "            oc_start = group * (self.out_channels // self.groups)\n",
    "            oc_end = oc_start + (self.out_channels // self.groups)\n",
    "            \n",
    "            for oc in range(oc_start, oc_end):\n",
    "                conv_value = 0.0\n",
    "                ic_in_group = c % (self.in_channels // self.groups)\n",
    "                for ky in range(self.kernel_size):\n",
    "                    for kx in range(self.kernel_size):\n",
    "                        yy = y + ky - self.kernel_size // 2\n",
    "                        xx = x + kx - self.kernel_size // 2\n",
    "                        if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                            weight = self.weight[oc, ic_in_group, ky, kx].item()\n",
    "                            mask = (padded_indices[0] == b) & (padded_indices[1] == c) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                            if mask.any():\n",
    "                                input_value = values[mask].sum().item()\n",
    "                                conv_value += weight * input_value\n",
    "\n",
    "                if self.bias is not None:\n",
    "                    conv_value += self.bias[oc].item()\n",
    "\n",
    "                out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                if out_y < 0 or out_y >= out_height or out_x < 0 or out_x >= out_width:\n",
    "                    continue\n",
    "                output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices, dtype=torch.long).t()\n",
    "        output_values = torch.tensor(output_values, dtype=torch.float)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    groups = 3\n",
    "\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups)\n",
    "    conv_layer.bias.data = torch.zeros_like(conv_layer.bias.data)\n",
    "\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    input_tensor_dense = torch.randn(10, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2dToMatMul:\n",
    "    def __init__(self, conv_layer):\n",
    "        self.conv_layer = conv_layer\n",
    "        self.weight = conv_layer.weight\n",
    "        self.bias = conv_layer.bias\n",
    "        self.stride = conv_layer.stride[0]\n",
    "        self.padding = conv_layer.padding[0]\n",
    "        self.groups = conv_layer.groups\n",
    "\n",
    "        self.out_channels, self.in_channels_per_group, self.kernel_h, self.kernel_w = self.weight.shape\n",
    "        self.in_channels = self.in_channels_per_group * self.groups\n",
    "        self.kernel_size = self.kernel_h  \n",
    "\n",
    "    def sparse_conv_to_matmul(self, x_sparse):\n",
    "        batch_size, _, height, width = x_sparse.size()\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        indices = x_sparse._indices()\n",
    "        values = x_sparse._values()\n",
    "        \n",
    "        padded_indices = indices.clone()cd \n",
    "        padded_indices[2:] += self.padding\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "\n",
    "        output_indices = []\n",
    "        output_values = []\n",
    "\n",
    "        # Iterate over spatial indices first\n",
    "        for y in range(padded_height):\n",
    "            for x in range(padded_width):\n",
    "                if y < self.kernel_size // 2 or y >= padded_height - self.kernel_size // 2:\n",
    "                    continue\n",
    "                if x < self.kernel_size // 2 or x >= padded_width - self.kernel_size // 2:\n",
    "                    continue\n",
    "\n",
    "                mask_spatial = (padded_indices[2] == y) & (padded_indices[3] == x)\n",
    "                if not mask_spatial.any():\n",
    "                    continue\n",
    "\n",
    "                relevant_indices = padded_indices[:, mask_spatial]\n",
    "                relevant_values = values[mask_spatial]\n",
    "\n",
    "                for i in range(relevant_values.size(0)):\n",
    "                    b, c = relevant_indices[0, i], relevant_indices[1, i]\n",
    "\n",
    "                    group = c // (self.in_channels // self.groups)\n",
    "                    oc_start = group * (self.out_channels // self.groups)\n",
    "                    oc_end = oc_start + (self.out_channels // self.groups)\n",
    "                    \n",
    "                    for oc in range(oc_start, oc_end):\n",
    "                        conv_value = 0.0\n",
    "                        ic_in_group = c % (self.in_channels // self.groups)\n",
    "                        for ky in range(self.kernel_size):\n",
    "                            for kx in range(self.kernel_size):\n",
    "                                yy = y + ky - self.kernel_size // 2\n",
    "                                xx = x + kx - self.kernel_size // 2\n",
    "                                if 0 <= yy < padded_height and 0 <= xx < padded_width:\n",
    "                                    weight = self.weight[oc, ic_in_group, ky, kx].item()\n",
    "                                    mask = (padded_indices[0] == b) & (padded_indices[1] == c) & (padded_indices[2] == yy) & (padded_indices[3] == xx)\n",
    "                                    if mask.any():\n",
    "                                        input_value = values[mask].sum().item()\n",
    "                                        conv_value += weight * input_value\n",
    "\n",
    "                        if self.bias is not None:\n",
    "                            conv_value += self.bias[oc].item()\n",
    "\n",
    "                        out_y = (y - self.kernel_size // 2) // self.stride\n",
    "                        out_x = (x - self.kernel_size // 2) // self.stride\n",
    "                        if out_y < 0 or out_y >= out_height or out_x < 0 or out_x >= out_width:\n",
    "                            continue\n",
    "                        output_indices.append([b.item(), oc, out_y, out_x])\n",
    "                        output_values.append(conv_value)\n",
    "\n",
    "        output_indices = torch.tensor(output_indices, dtype=torch.long).t()\n",
    "        output_values = torch.tensor(output_values, dtype=torch.float)\n",
    "\n",
    "        output_shape = (batch_size, self.out_channels, out_height, out_width)\n",
    "        output_sparse = torch.sparse.FloatTensor(output_indices, output_values, output_shape)\n",
    "        return output_sparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_channels = 3\n",
    "    out_channels = 12\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    groups = 3\n",
    "\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups)\n",
    "    conv_layer.bias.data = torch.zeros_like(conv_layer.bias.data)\n",
    "\n",
    "    sparse_conv_to_matmul = SparseConv2dToMatMul(conv_layer)\n",
    "\n",
    "    input_tensor_dense = torch.randn(10, in_channels, 5, 5)\n",
    "    input_tensor_sparse = input_tensor_dense.to_sparse()\n",
    "\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(input_tensor_sparse)\n",
    "    conv_output = conv_layer(input_tensor_dense)\n",
    "\n",
    "    diff = torch.sum(matmul_output_sparse.to_dense() - conv_output)\n",
    "    print(\"Différence entre les résultats:\", diff.item())\n",
    "    print(\"Résultat du produit matriciel pour tenseur sparse:\", matmul_output_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('app/src')\n",
    "sys.path.append('./src')\n",
    "from zono_sparse_gen import ZonoSparseGeneration\n",
    "test_input = torch.randn(3,112,112)\n",
    "_,zonotope_espilon_sparse_tensor = ZonoSparseGeneration(test_input,0.01).total_zono()\n",
    "\n",
    "print(zonotope_espilon_sparse_tensor)\n",
    "with torch.no_grad():\n",
    "    matmul_output_sparse = sparse_conv_to_matmul.sparse_conv_to_matmul(zonotope_espilon_sparse_tensor)\n",
    "print(matmul_output_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = jax.random.normal(jax.random.PRNGKey(0), (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = jax.random.normal(jax.random.PRNGKey(1), (out_channels,))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, W, H)\n",
    "        Returns:\n",
    "            The result of the convolution as a JAX array\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "        \n",
    "        B, C, W, H = sparse_tensor.shape\n",
    "        \n",
    "        # Initialize output shape\n",
    "        out_height = (W - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        out_width = (H - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        output = np.zeros((B, self.weights.shape[0], out_height, out_width))\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, w, h = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "            for k in range(self.weights.shape[0]):\n",
    "                for kh in range(self.kernel_size):\n",
    "                    for kw in range(self.kernel_size):\n",
    "                        h_out = (h - kw + self.padding) // self.stride\n",
    "                        w_out = (w - kh + self.padding) // self.stride\n",
    "                        if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                            output[b, k, h_out, w_out] += value * self.weights[k, c, kh, kw]\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias[:, None, None]\n",
    "\n",
    "        return jnp.array(output)\n",
    "\n",
    "# Example usage:\n",
    "B, C, W, H = 1, 3, 5, 5\n",
    "data = torch.sparse_coo_tensor(indices=[[0, 0, 0, 1], [0, 1, 2, 0], [1, 2, 3, 4], [1, 2, 3, 4]],\n",
    "                               values=[1, 2, 3, 4],\n",
    "                               size=[B, C, W, H])\n",
    "\n",
    "conv = SparseConv2D(in_channels=3, out_channels=2, kernel_size=3)\n",
    "result = conv(data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = jax.random.normal(jax.random.PRNGKey(0), (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = jax.random.normal(jax.random.PRNGKey(1), (out_channels,))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, W, H)\n",
    "        Returns:\n",
    "            The result of the convolution as a JAX array\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "        \n",
    "        B, C, W, H = sparse_tensor.shape\n",
    "        \n",
    "        # Initialize output shape\n",
    "        out_height = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output = np.zeros((B, self.weights.shape[0], out_height, out_width))\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, w, h = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "            for k in range(self.weights.shape[0]):\n",
    "                for kh in range(self.kernel_size):\n",
    "                    for kw in range(self.kernel_size):\n",
    "                        h_out = (h - kw + self.padding) // self.stride\n",
    "                        w_out = (w - kh + self.padding) // self.stride\n",
    "                        if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                            output[b, k, h_out, w_out] += value * self.weights[k, c, kh, kw]\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias[:, None, None]\n",
    "\n",
    "        return jnp.array(output)\n",
    "\n",
    "# Example usage:\n",
    "B, C, W, H = 100000, 3, 5, 5\n",
    "data = torch.sparse_coo_tensor(indices=[[0, 0, 0, 0], [0, 1, 2, 0], [1, 2, 3, 4], [1, 2, 3, 4]],\n",
    "                               values=[1, 2, 3, 4],\n",
    "                               size=[B, C, W, H])\n",
    "\n",
    "conv = SparseConv2D(in_channels=3, out_channels=2, kernel_size=3)\n",
    "result = conv(data)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, weights=None, bias_val=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if weights is None:\n",
    "            self.weights = jax.random.normal(jax.random.PRNGKey(0), (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if bias:\n",
    "            if bias_val is None:\n",
    "                self.bias = jax.random.normal(jax.random.PRNGKey(1), (out_channels,))\n",
    "            else:\n",
    "                self.bias = bias_val\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, W, H)\n",
    "        Returns:\n",
    "            The result of the convolution as a JAX array\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "        \n",
    "        B, C, W, H = sparse_tensor.shape\n",
    "        \n",
    "        # Initialize output shape\n",
    "        out_height = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output = np.zeros((B, self.weights.shape[0], out_height, out_width))\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, w, h = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "            for k in range(self.weights.shape[0]):\n",
    "                for kh in range(self.kernel_size):\n",
    "                    for kw in range(self.kernel_size):\n",
    "                        h_out = (h - kw + self.padding) // self.stride\n",
    "                        w_out = (w - kh + self.padding) // self.stride\n",
    "                        if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                            output[b, k, h_out, w_out] += value * self.weights[k, c, kh, kw]\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias[:, None, None]\n",
    "\n",
    "        return jnp.array(output)\n",
    "\n",
    "def conv2d_to_sparseconv2d(conv2d):\n",
    "    \"\"\"\n",
    "    Transforms a torch.nn.Conv2d instance to a SparseConv2D instance\n",
    "    \n",
    "    Args:\n",
    "        conv2d: instance of torch.nn.Conv2d\n",
    "    \n",
    "    Returns:\n",
    "        instance of SparseConv2D\n",
    "    \"\"\"\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size = conv2d.kernel_size[0]\n",
    "    stride = conv2d.stride[0]\n",
    "    padding = conv2d.padding[0]\n",
    "    weights = jnp.array(conv2d.weight.detach().numpy())\n",
    "    if conv2d.bias is not None:\n",
    "        bias = jnp.array(conv2d.bias.detach().numpy())\n",
    "    else:\n",
    "        bias = None\n",
    "    \n",
    "    return SparseConv2D(in_channels, out_channels, kernel_size, stride, padding, bias=(bias is not None), weights=weights, bias_val=bias)\n",
    "\n",
    "# Example usage:\n",
    "conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "sparse_conv2d = conv2d_to_sparseconv2d(conv2d)\n",
    "\n",
    "# Creating a sparse tensor for testing\n",
    "B, C, W, H = 100, 3, 5, 5\n",
    "data = torch.sparse_coo_tensor(indices=[[0, 0, 0, 0], [0, 1, 2, 0], [1, 2, 3, 4], [1, 2, 3, 4]],\n",
    "                               values=[1., 2., 3., 4.],\n",
    "                               size=[B, C, W, H])\n",
    "\n",
    "# Applying the sparse convolution\n",
    "result = sparse_conv2d(data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, weights=None, bias_val=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if weights is None:\n",
    "            self.weights = jax.random.normal(jax.random.PRNGKey(0), (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if bias:\n",
    "            if bias_val is None:\n",
    "                self.bias = jax.random.normal(jax.random.PRNGKey(1), (out_channels,))\n",
    "            else:\n",
    "                self.bias = bias_val\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, W, H)\n",
    "        Returns:\n",
    "            The result of the convolution as a sparse tensor in COO format\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "        \n",
    "        B, C, W, H = sparse_tensor.shape\n",
    "        \n",
    "        # Initialize output shape\n",
    "        out_height = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output_values = []\n",
    "        output_indices = []\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, w, h = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "            for k in range(self.weights.shape[0]):\n",
    "                for kh in range(self.kernel_size):\n",
    "                    for kw in range(self.kernel_size):\n",
    "                        h_out = (h - kw + self.padding) // self.stride\n",
    "                        w_out = (w - kh + self.padding) // self.stride\n",
    "                        if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                            output_values.append(value * self.weights[k, c, kh, kw])\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        if self.bias is not None:\n",
    "            for b in range(B):\n",
    "                for k in range(self.weights.shape[0]):\n",
    "                    for h_out in range(out_height):\n",
    "                        for w_out in range(out_width):\n",
    "                            output_values.append(self.bias[k])\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        output_values = np.array(output_values)\n",
    "        output_indices = np.array(output_indices).T\n",
    "        \n",
    "        size = (B, self.weights.shape[0], out_height, out_width)\n",
    "        sparse_output = torch.sparse_coo_tensor(output_indices, output_values, size)\n",
    "        \n",
    "        return sparse_output\n",
    "\n",
    "def conv2d_to_sparseconv2d(conv2d):\n",
    "    \"\"\"\n",
    "    Transforms a torch.nn.Conv2d instance to a SparseConv2D instance\n",
    "    \n",
    "    Args:\n",
    "        conv2d: instance of torch.nn.Conv2d\n",
    "    \n",
    "    Returns:\n",
    "        instance of SparseConv2D\n",
    "    \"\"\"\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size = conv2d.kernel_size[0]\n",
    "    stride = conv2d.stride[0]\n",
    "    padding = conv2d.padding[0]\n",
    "    weights = jnp.array(conv2d.weight.detach().numpy())\n",
    "    if conv2d.bias is not None:\n",
    "        bias = jnp.array(conv2d.bias.detach().numpy())\n",
    "    else:\n",
    "        bias = None\n",
    "    \n",
    "    return SparseConv2D(in_channels, out_channels, kernel_size, stride, padding, bias=(bias is not None), weights=weights, bias_val=bias)\n",
    "\n",
    "# Example usage:\n",
    "conv2d = torch.nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "conv2d.bias = None\n",
    "sparse_conv2d = conv2d_to_sparseconv2d(conv2d)\n",
    "\n",
    "# Creating a sparse tensor for testing\n",
    "B, C, W, H = 150000, 3, 5, 5\n",
    "data = torch.sparse_coo_tensor(indices=[[0, 0, 0, 0], [0, 1, 2, 0], [1, 2, 3, 4], [1, 2, 3, 4]],\n",
    "                               values=[1., 2., 3., 4.],\n",
    "                               size=[B, C, W, H])\n",
    "\n",
    "# Applying the sparse convolution\n",
    "result = sparse_conv2d(data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dense = data.to_dense()\n",
    "print(data_dense.shape)\n",
    "dense_conv = conv2d(data_dense)\n",
    "print(dense_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dense_conv-result.to_dense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[  0,   0,   0,  ...,   0,   0,   0],\n",
      "                       [  0,   0,   0,  ..., 127, 127, 127],\n",
      "                       [  0,   0,   0,  ...,   5,   5,   5],\n",
      "                       [  0,   1,   2,  ...,   3,   4,   5]]),\n",
      "       values=tensor([ 0.0047,  0.0740,  0.0823,  ..., -0.1086, -0.7443,\n",
      "                       0.7202]),\n",
      "       size=(15, 128, 224, 224), nnz=3072, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, weights=None, bias_val=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if weights is None:\n",
    "            self.weights = torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "        \n",
    "        if bias:\n",
    "            if bias_val is None:\n",
    "                self.bias = torch.randn(out_channels)\n",
    "            else:\n",
    "                self.bias = torch.tensor(bias_val, dtype=torch.float32)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, W, H)\n",
    "        Returns:\n",
    "            The result of the convolution as a sparse tensor in COO format\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "        \n",
    "        B, C, W, H = sparse_tensor.shape\n",
    "        out_height = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output_values = []\n",
    "        output_indices = []\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, w, h = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "            for k in range(self.weights.shape[0]):\n",
    "                for kh in range(self.kernel_size):\n",
    "                    for kw in range(self.kernel_size):\n",
    "                        h_out = (h - kw + self.padding) // self.stride\n",
    "                        w_out = (w - kh + self.padding) // self.stride\n",
    "                        if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                            output_values.append(value * self.weights[k, c, kh, kw].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        if self.bias is not None:\n",
    "            for b in range(B):\n",
    "                for k in range(self.weights.shape[0]):\n",
    "                    for h_out in range(out_height):\n",
    "                        for w_out in range(out_width):\n",
    "                            output_values.append(self.bias[k].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        output_values = torch.tensor(output_values)\n",
    "        output_indices = torch.tensor(output_indices).T\n",
    "        \n",
    "        size = (B, self.weights.shape[0], out_height, out_width)\n",
    "        sparse_output = torch.sparse_coo_tensor(output_indices, output_values, size).coalesce()\n",
    "        \n",
    "        return sparse_output\n",
    "\n",
    "def conv2d_to_sparseconv2d(conv2d):\n",
    "    \"\"\"\n",
    "    Transforms a torch.nn.Conv2d instance to a SparseConv2D instance\n",
    "    \n",
    "    Args:\n",
    "        conv2d: instance of torch.nn.Conv2d\n",
    "    \n",
    "    Returns:\n",
    "        instance of SparseConv2D\n",
    "    \"\"\"\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size = conv2d.kernel_size[0]\n",
    "    stride = conv2d.stride[0]\n",
    "    padding = conv2d.padding[0]\n",
    "    weights = conv2d.weight.detach().numpy()\n",
    "    if conv2d.bias is not None:\n",
    "        bias = conv2d.bias.detach().numpy()\n",
    "    else:\n",
    "        bias = None\n",
    "    \n",
    "    return SparseConv2D(in_channels, out_channels, kernel_size, stride, padding, bias=(bias is not None), weights=weights, bias_val=bias)\n",
    "\n",
    "# Example usage:\n",
    "conv2d = torch.nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "conv2d.bias = None\n",
    "sparse_conv2d = conv2d_to_sparseconv2d(conv2d)\n",
    "\n",
    "\n",
    "# Creating a sparse tensor for testing\n",
    "B, C, W, H = 15, 3,224,224\n",
    "data = torch.sparse_coo_tensor(indices=[[0, 0, 0, 0], [0, 1, 2, 0], [1, 2, 3, 4], [1, 2, 3, 4]],\n",
    "                               values=[1., 2., 3., 4.],\n",
    "                               size=[B, C, W, H])\n",
    "\n",
    "# Applying the sparse convolution\n",
    "result = sparse_conv2d(data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3, 3])\n",
      "out height 3\n",
      "out_width 3\n",
      "SparseConv2D result (sparse): tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.2160],\n",
      "          [ 0.0000,  0.0000,  0.7975]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0127],\n",
      "          [ 0.0000,  0.0000, -1.0210]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.3500],\n",
      "          [ 0.0000,  0.0000,  0.1905]]]])\n",
      "Conv2D result (dense): tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0286,  0.3873],\n",
      "          [ 0.0000,  0.0736,  0.1582]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.3039, -0.2102],\n",
      "          [ 0.0000,  0.0895, -0.2971]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.3208, -0.3539],\n",
      "          [ 0.0000,  0.1880, -0.1693]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, bias=True, weights=None, bias_val=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.dilatation = 1\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if weights is None:\n",
    "            self.weights = torch.randn(out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "        \n",
    "        if bias:\n",
    "            if bias_val is None:\n",
    "                self.bias = torch.randn(out_channels)\n",
    "            else:\n",
    "                self.bias = torch.tensor(bias_val, dtype=torch.float32)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, H, W)\n",
    "        Returns:\n",
    "            The result of the convolution as a sparse tensor in COO format\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "        \n",
    "        B, C, H, W = sparse_tensor.shape\n",
    "        out_height = (H + 2 * self.padding - self.dilatation*(self.kernel_size-1)-1) // self.stride + 1\n",
    "        out_width = (W + 2 * self.padding - self.dilatation *(self.kernel_size-1)-1) // self.stride + 1\n",
    "        output_values = []\n",
    "        output_indices = []\n",
    "        print(self.weights.shape)\n",
    "        print('out height', out_height)\n",
    "        print('out_width',out_width)\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, h, w = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "        \n",
    "            for kh in range(self.kernel_size):\n",
    "                for kw in range(self.kernel_size):\n",
    "                    h_out = (h - kh + 2*self.padding) // self.stride\n",
    "                    w_out = (w - kw + 2*self.padding) // self.stride\n",
    "                    if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                        for k in range(self.weights.shape[0]):\n",
    "                            output_values.append(value * self.weights[k, (c % (C // self.groups)), kh, kw].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        # Apply bias\n",
    "        if self.bias is not None:\n",
    "            for b in range(B):\n",
    "                for k in range(self.weights.shape[0]):\n",
    "                    for h_out in range(out_height):\n",
    "                        for w_out in range(out_width):\n",
    "                            output_values.append(self.bias[k].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        output_values = torch.tensor(output_values)\n",
    "        output_indices = torch.tensor(output_indices).T\n",
    "        \n",
    "        size = (B, self.weights.shape[0], out_height, out_width)\n",
    "        sparse_output = torch.sparse_coo_tensor(output_indices, output_values, size)\n",
    "        \n",
    "        return sparse_output\n",
    "\n",
    "def conv2d_to_sparseconv2d(conv2d):\n",
    "    \"\"\"\n",
    "    Transforms a torch.nn.Conv2d instance to a SparseConv2D instance\n",
    "    \n",
    "    Args:\n",
    "        conv2d: instance of torch.nn.Conv2d\n",
    "    \n",
    "    Returns:\n",
    "        instance of SparseConv2D\n",
    "    \"\"\"\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size = conv2d.kernel_size[0]\n",
    "    stride = conv2d.stride[0]\n",
    "    padding = conv2d.padding[0]\n",
    "    groups = conv2d.groups\n",
    "    weights = conv2d.weight.detach().numpy()\n",
    "    if conv2d.bias is not None:\n",
    "        bias = conv2d.bias.detach().numpy()\n",
    "    else:\n",
    "        bias = None\n",
    "    \n",
    "    return SparseConv2D(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=(bias is not None), weights=weights, bias_val=bias)\n",
    "\n",
    "# Example usage:\n",
    "conv2d = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=2, padding=2, groups=1)\n",
    "\n",
    "conv2d.bias = None\n",
    "sparse_conv2d = conv2d_to_sparseconv2d(conv2d)\n",
    "\n",
    "# Creating a sparse tensor for testing\n",
    "B, C, H, W = 1, 3, 4, 4\n",
    "indices = torch.tensor([[0, 0, 0, 1], [0, 1, 2, 0], [2, 1, 3, 4], [2, 2, 3, 4]])\n",
    "values = torch.tensor([1., 2., 3., 4.])\n",
    "data = torch.sparse_coo_tensor(indices, values, size=[B, C, H, W])\n",
    "\n",
    "# Applying the sparse convolution\n",
    "result = sparse_conv2d(data)\n",
    "\n",
    "# For comparison, dense convolution on a dense version of the sparse tensor\n",
    "dense_data = data.to_dense()\n",
    "conv2d_result = conv2d(dense_data)\n",
    "\n",
    "print(\"SparseConv2D result (sparse):\", result.to_dense())\n",
    "print(\"Conv2D result (dense):\", conv2d_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, bias=True, weights=None, bias_val=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        if weights is None:\n",
    "            self.weights = torch.randn(out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "        if bias:\n",
    "            if bias_val is None:\n",
    "                self.bias = torch.randn(out_channels)\n",
    "            else:\n",
    "                self.bias = torch.tensor(bias_val, dtype=torch.float32)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, H, W)\n",
    "        Returns:\n",
    "            The result of the convolution as a sparse tensor in COO format\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "\n",
    "        B, C, H, W = sparse_tensor.shape\n",
    "        out_height = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output_values = []\n",
    "        output_indices = []\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, h, w = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "            for kh in range(self.kernel_size):\n",
    "                for kw in range(self.kernel_size):\n",
    "                    h_out = (h + self.padding - kh) // self.stride\n",
    "                    w_out = (w + self.padding - kw) // self.stride\n",
    "                    if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                        for k in range(self.weights.shape[0]):\n",
    "                            output_values.append(value * self.weights[k, c % (C // self.groups), kh, kw].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        # Apply bias\n",
    "        if self.bias is not None:\n",
    "            for b in range(B):\n",
    "                for k in range(self.weights.shape[0]):\n",
    "                    for h_out in range(out_height):\n",
    "                        for w_out in range(out_width):\n",
    "                            output_values.append(self.bias[k].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        output_values = torch.tensor(output_values)\n",
    "        output_indices = torch.tensor(output_indices).T\n",
    "\n",
    "        size = (B, self.weights.shape[0], out_height, out_width)\n",
    "        sparse_output = torch.sparse_coo_tensor(output_indices, output_values, size).coalesce()\n",
    "\n",
    "        return sparse_output\n",
    "\n",
    "def conv2d_to_sparseconv2d(conv2d):\n",
    "    \"\"\"\n",
    "    Transforms a torch.nn.Conv2d instance to a SparseConv2D instance\n",
    "\n",
    "    Args:\n",
    "        conv2d: instance of torch.nn.Conv2d\n",
    "\n",
    "    Returns:\n",
    "        instance of SparseConv2D\n",
    "    \"\"\"\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size = conv2d.kernel_size[0]\n",
    "    stride = conv2d.stride[0]\n",
    "    padding = conv2d.padding[0]\n",
    "    groups = conv2d.groups\n",
    "    weights = conv2d.weight.detach().numpy()\n",
    "    if conv2d.bias is not None:\n",
    "        bias = conv2d.bias.detach().numpy()\n",
    "    else:\n",
    "        bias = None\n",
    "\n",
    "    return SparseConv2D(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=(bias is not None), weights=weights, bias_val=bias)\n",
    "\n",
    "# Example usage:\n",
    "conv2d = torch.nn.Conv2d(in_channels=3, out_channels=18, kernel_size=3, stride=1, padding=1, groups=3)\n",
    "conv2d.bias = None\n",
    "sparse_conv2d = conv2d_to_sparseconv2d(conv2d)\n",
    "\n",
    "# Creating a sparse tensor for testing\n",
    "B, C, H, W = 1, 3, 5, 5\n",
    "indices = torch.tensor([[0, 0, 0, 1], [0, 1, 2, 0], [2, 1, 3, 4], [2, 2, 3, 4]])\n",
    "values = torch.tensor([1., 2., 3., 4.])\n",
    "data = torch.sparse_coo_tensor(indices, values, size=[B, C, H, W])\n",
    "\n",
    "# Applying the sparse convolution\n",
    "result = sparse_conv2d(data)\n",
    "\n",
    "# For comparison, dense convolution on a dense version of the sparse tensor\n",
    "dense_data = data.to_dense()\n",
    "conv2d_result = conv2d(dense_data)\n",
    "\n",
    "print(\"SparseConv2D result (sparse):\", result.to_dense())\n",
    "print(\"Conv2D result (dense):\", conv2d_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de valeurs non nulles: 0.16%\n",
      "Gain en pourcentage de mémoire: 99.20%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def sparse_tensor_stats(sparse_tensor):\n",
    "    # Vérifie si le tenseur est au format COO\n",
    "    if sparse_tensor.layout != torch.sparse_coo:\n",
    "        raise ValueError(\"Le tenseur doit être au format COO\")\n",
    "\n",
    "    # Calculer le nombre total d'éléments dans le tenseur\n",
    "    total_elements = sparse_tensor.size(0) * sparse_tensor.size(1)\n",
    "    \n",
    "    # Calculer le nombre d'éléments non nuls\n",
    "    non_zero_elements = sparse_tensor._nnz()\n",
    "    \n",
    "    # Calculer le pourcentage de valeurs non nulles\n",
    "    non_zero_percentage = (non_zero_elements / total_elements) * 100\n",
    "    \n",
    "    # Calculer la mémoire utilisée par le tenseur dense\n",
    "    dense_memory = total_elements * sparse_tensor.dtype.itemsize\n",
    "    \n",
    "    # Calculer la mémoire utilisée par le tenseur sparse\n",
    "    values_memory = sparse_tensor.values().numel() * sparse_tensor.values().element_size()\n",
    "    indices_memory = sparse_tensor.indices().numel() * sparse_tensor.indices().element_size()\n",
    "    sparse_memory = values_memory + indices_memory\n",
    "    \n",
    "    # Calculer le gain en pourcentage de mémoire\n",
    "    memory_gain_percentage = ((dense_memory - sparse_memory) / dense_memory) * 100\n",
    "    \n",
    "    return non_zero_percentage, memory_gain_percentage\n",
    "\n",
    "# Exemple d'utilisation\n",
    "indices = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 3]])\n",
    "values = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices, values, (50, 50)).coalesce()\n",
    "\n",
    "non_zero_percentage, memory_gain_percentage = sparse_tensor_stats(sparse_tensor)\n",
    "print(f\"Pourcentage de valeurs non nulles: {non_zero_percentage:.2f}%\")\n",
    "print(f\"Gain en pourcentage de mémoire: {memory_gain_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.zeros(3,2,24,24)\n",
    "torch.sum(x,dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
