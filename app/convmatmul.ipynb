{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/Documents/AbstratRay/app/./src/zono_sparse_gen.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(tensor)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('app/src')\n",
    "sys.path.append('./src')\n",
    "from zono_sparse_gen import ZonoSparseGeneration\n",
    "test_input = torch.randn(3,224,224)\n",
    "_,zonotope_espilon_sparse_tensor = ZonoSparseGeneration(test_input,0.01).total_zono()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[     0,      1,      2,  ..., 150525, 150526, 150527],\n",
      "                       [     0,      0,      0,  ...,      2,      2,      2],\n",
      "                       [     0,      0,      0,  ...,    223,    223,    223],\n",
      "                       [     0,      1,      2,  ...,    221,    222,    223]]),\n",
      "       values=tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]),\n",
      "       size=(150528, 3, 224, 224), nnz=150528, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/Documents/AbstratRay/app/./src/zono_sparse_gen.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(tensor)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('app/src')\n",
    "sys.path.append('./src')\n",
    "from zono_sparse_gen import ZonoSparseGeneration\n",
    "test_input = torch.randn(3,224,224)\n",
    "_,zonotope_espilon_sparse_tensor = ZonoSparseGeneration(test_input,0.01).total_zono()\n",
    "\n",
    "print(zonotope_espilon_sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.1048e-07, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, dilation =1, bias=True, weights=None, bias_val=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        if weights is None:\n",
    "            self.weights = torch.randn(out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "        \n",
    "        if bias:\n",
    "            if bias_val is None:\n",
    "                self.bias = torch.randn(out_channels)\n",
    "            else:\n",
    "                self.bias = torch.tensor(bias_val, dtype=torch.float32)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, H, W)\n",
    "        Returns:\n",
    "            The result of the convolution as a sparse tensor in COO format\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "        \n",
    "        B, C, H, W = sparse_tensor.shape\n",
    "        out_height = (H + 2 * self.padding - self.dilation*(self.kernel_size-1)-1) // self.stride + 1\n",
    "        out_width = (W + 2 * self.padding - self.dilation *(self.kernel_size-1)-1) // self.stride + 1\n",
    "        output_values = []\n",
    "        output_indices = []\n",
    "        \n",
    "\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "\n",
    "            b, c, h, w = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "           \n",
    "            for kh in range(self.kernel_size):\n",
    "                for kw in range(self.kernel_size):\n",
    "                    h_out = ((h + self.padding - kh * self.dilation)+1) // self.stride -1\n",
    "                    w_out = ((w + self.padding - kw * self.dilation)+1) // self.stride -1\n",
    "                    if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                        for k in range(self.weights.shape[0]):\n",
    "                        \n",
    "                            output_values.append(value * self.weights[k, (c % (C // self.groups)), kh, kw].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        # Apply bias\n",
    "        if self.bias is not None:\n",
    "            for b in range(B):\n",
    "                for k in range(self.weights.shape[0]):\n",
    "                    for h_out in range(out_height):\n",
    "                        for w_out in range(out_width):\n",
    "                            output_values.append(self.bias[k].item())\n",
    "                            output_indices.append([b, k, h_out, w_out])\n",
    "\n",
    "        output_values = torch.tensor(output_values)\n",
    "        output_indices = torch.tensor(output_indices).T\n",
    "        \n",
    "        size = (B, self.weights.shape[0], out_height, out_width)\n",
    "        sparse_output = torch.sparse_coo_tensor(output_indices, output_values, size)\n",
    "        \n",
    "        return sparse_output\n",
    "\n",
    "def conv2d_to_sparseconv2d(conv2d):\n",
    "    \"\"\"\n",
    "    Transforms a torch.nn.Conv2d instance to a SparseConv2D instance\n",
    "    \n",
    "    Args:\n",
    "        conv2d: instance of torch.nn.Conv2d\n",
    "    \n",
    "    Returns:\n",
    "        instance of SparseConv2D\n",
    "    \"\"\"\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size = conv2d.kernel_size[0]\n",
    "    stride = conv2d.stride[0]\n",
    "    padding = conv2d.padding[0]\n",
    "    groups = conv2d.groups\n",
    "    weights = conv2d.weight.detach().numpy()\n",
    "    dilation = conv2d.dilation[0]\n",
    "    if conv2d.bias is not None:\n",
    "        bias = conv2d.bias.detach().numpy()\n",
    "    else:\n",
    "        bias = None\n",
    "    \n",
    "    return SparseConv2D(in_channels, out_channels, kernel_size, stride, padding, groups=groups, dilation =dilation,bias=(bias is not None), weights=weights, bias_val=bias)\n",
    "\n",
    "# Example usage:\n",
    "conv2d = torch.nn.Conv2d(in_channels=3, out_channels=30, kernel_size=3, stride=1, padding=0, groups=1,dilation=1)\n",
    "\n",
    "conv2d.bias = None\n",
    "sparse_conv2d = conv2d_to_sparseconv2d(conv2d)\n",
    "\n",
    "# Creating a sparse tensor for testing\n",
    "B, C, H, W = 3, 3, 10, 10\n",
    "indices = torch.tensor([[0, 0, 0, 0], [0, 0, 2, 0], [2, 1, 3, 0], [2, 1, 3, 0]])\n",
    "values = torch.tensor([1., 2., 3., 4.])\n",
    "data = torch.sparse_coo_tensor(indices, values, size=[B, C, H, W])\n",
    "\n",
    "# Applying the sparse convolution\n",
    "result = sparse_conv2d(data)\n",
    "\n",
    "# For comparison, dense convolution on a dense version of the sparse tensor\n",
    "dense_data = data.to_dense()\n",
    "conv2d_result = conv2d(dense_data)\n",
    "\n",
    "\n",
    "\n",
    "print(torch.sum(conv2d_result-result.to_dense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_2 \u001b[38;5;241m=\u001b[39m\u001b[43msparse_conv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzonotope_espilon_sparse_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[112], line 59\u001b[0m, in \u001b[0;36mSparseConv2D.__call__\u001b[0;34m(self, sparse_tensor)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m h_out \u001b[38;5;241m<\u001b[39m out_height \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m w_out \u001b[38;5;241m<\u001b[39m out_width:\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 59\u001b[0m                     output_values\u001b[38;5;241m.\u001b[39mappend(value \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     60\u001b[0m                     output_indices\u001b[38;5;241m.\u001b[39mappend([b, k, h_out, w_out])\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Apply bias\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_2 =sparse_conv2d(zonotope_espilon_sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 37631, 37631, 37631],\n",
       "                       [    0,     1,     2,  ...,     0,     1,     2],\n",
       "                       [    0,     0,     0,  ...,   109,   109,   109],\n",
       "                       [    0,     0,     0,  ...,   109,   109,   109]]),\n",
       "       values=tensor([-0.0001,  0.0010,  0.0004,  ..., -0.0006, -0.0013,\n",
       "                       0.0006]),\n",
       "       size=(37632, 3, 110, 110), nnz=980100, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  1,  2,  ..., 27, 28, 29],\n",
      "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        [ 1,  1,  1,  ..., 23, 23, 23]])\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseConv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, dilation=1, bias=True, weights=None, bias_val=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        print(self.stride)\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        if weights is None:\n",
    "            self.weights = torch.randn(out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "        if bias:\n",
    "            if bias_val is None:\n",
    "                self.bias = torch.randn(out_channels)\n",
    "            else:\n",
    "                self.bias = torch.tensor(bias_val, dtype=torch.float32)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def __call__(self, sparse_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sparse_tensor: a torch sparse tensor in COO format with shape (B, C, H, W)\n",
    "        Returns:\n",
    "            The result of the convolution as a sparse tensor in COO format\n",
    "        \"\"\"\n",
    "        coo = sparse_tensor.coalesce()\n",
    "        values = coo.values()\n",
    "        indices = coo.indices()\n",
    "\n",
    "        B, C, H, W = sparse_tensor.shape\n",
    "        out_height = (H + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n",
    "        out_width = (W + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n",
    "        \n",
    "        output = {\n",
    "            'values': [],\n",
    "            'indices': []\n",
    "        }\n",
    "\n",
    "        # Iterate over the non-zero elements in the sparse tensor\n",
    "        for i in range(values.shape[0]):\n",
    "            b, c, h, w = indices[:, i].tolist()\n",
    "            value = values[i].item()\n",
    "\n",
    "            for kh in range(self.kernel_size):\n",
    "                for kw in range(self.kernel_size):\n",
    "                    h_out = (h + self.padding - kh * self.dilation) // self.stride\n",
    "                    w_out = (w + self.padding - kw * self.dilation) // self.stride\n",
    "                    \n",
    "\n",
    "                    if 0 <= h_out < out_height and 0 <= w_out < out_width:\n",
    "                        for k in range(self.weights.shape[0]):\n",
    "                            group_idx = c // (C // self.groups)\n",
    "                            weight_idx = c % (C // self.groups)\n",
    "                            conv_value = value * self.weights[k, weight_idx, kh, kw].item()\n",
    "\n",
    "                            output['values'].append(conv_value)\n",
    "                            output['indices'].append([b, k, h_out, w_out])\n",
    "\n",
    "        # Apply bias\n",
    "        if self.bias is not None:\n",
    "            for b in range(B):\n",
    "                for k in range(self.weights.shape[0]):\n",
    "                    for h_out in range(out_height):\n",
    "                        for w_out in range(out_width):\n",
    "                            output['values'].append(self.bias[k].item())\n",
    "                            output['indices'].append([b, k, h_out, w_out])\n",
    "\n",
    "        output['values'] = torch.tensor(output['values'])\n",
    "        output['indices'] = torch.tensor(output['indices']).T\n",
    "        print(output['indices'])\n",
    "\n",
    "        size = (B, self.weights.shape[0], out_height, out_width)\n",
    "        sparse_output = torch.sparse_coo_tensor(output['indices'], output['values'], size)\n",
    "\n",
    "        return sparse_output,torch.sum(torch.abs(sparse_output),dim=0)\n",
    "\n",
    "def conv2d_to_sparseconv2d(conv2d):\n",
    "    \"\"\"\n",
    "    Transforms a torch.nn.Conv2d instance to a SparseConv2D instance\n",
    "    \n",
    "    Args:\n",
    "        conv2d: instance of torch.nn.Conv2d\n",
    "    \n",
    "    Returns:\n",
    "        instance of SparseConv2D\n",
    "    \"\"\"\n",
    "    in_channels = conv2d.in_channels\n",
    "    out_channels = conv2d.out_channels\n",
    "    kernel_size = conv2d.kernel_size[0]\n",
    "    stride = conv2d.stride[0]\n",
    "    padding = conv2d.padding[0]\n",
    "    groups = conv2d.groups\n",
    "    weights = conv2d.weight.detach().numpy()\n",
    "    dilation = conv2d.dilation[0]\n",
    "    if conv2d.bias is not None:\n",
    "        bias = conv2d.bias.detach().numpy()\n",
    "    else:\n",
    "        bias = None\n",
    "\n",
    "    return SparseConv2D(in_channels, out_channels, kernel_size, stride, padding, groups=groups, dilation=dilation, bias=(bias is not None), weights=weights, bias_val=bias)\n",
    "# Example usage:\n",
    "conv2d = torch.nn.Conv2d(in_channels=3, out_channels=30, kernel_size=3, stride=1\n",
    "                         , padding=0, groups=1,dilation=1)\n",
    "\n",
    "conv2d.bias = None\n",
    "sparse_conv2d = conv2d_to_sparseconv2d(conv2d)\n",
    "\n",
    "# Creating a sparse tensor for testing\n",
    "B, C, H, W = 3, 3, 50, 50\n",
    "indices = torch.tensor([[0, 0, 0, 0], [0, 0, 2, 2], [2, 1, 3, 0], [2, 1,25, 25]])\n",
    "values = torch.tensor([1., 2., 3., 4.])\n",
    "data = torch.sparse_coo_tensor(indices, values, size=[B, C, H, W])\n",
    "\n",
    "# Applying the sparse convolution\n",
    "result,sum = sparse_conv2d(data)\n",
    "\n",
    "# For comparison, dense convolution on a dense version of the sparse tensor\n",
    "dense_data = data.to_dense()\n",
    "conv2d_result = conv2d(dense_data)\n",
    "\n",
    "\n",
    "\n",
    "print(torch.sum(conv2d_result-result.to_dense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  0,  0,  ..., 29, 29, 29],\n",
       "                       [ 0,  0,  0,  ...,  3,  3,  3],\n",
       "                       [ 0,  1,  2,  ..., 23, 24, 25]]),\n",
       "       values=tensor([1.3801e-01, 2.0934e-01, 2.9606e-02, 5.0667e-02,\n",
       "                      3.2831e-01, 1.4538e-01, 1.7595e-01, 3.3262e-01,\n",
       "                      1.1319e-01, 2.2810e-01, 2.2187e-03, 1.6033e-01,\n",
       "                      6.7272e-02, 8.8782e-02, 1.6036e-01, 3.5208e-01,\n",
       "                      4.1958e-01, 5.7243e-01, 3.8000e-02, 2.4624e-01,\n",
       "                      1.0904e-01, 1.7704e-01, 1.3664e-01, 1.3593e-01,\n",
       "                      5.6017e-01, 1.9029e-01, 4.1737e-01, 1.4186e-01,\n",
       "                      1.2469e-01, 7.6905e-02, 2.7034e-01, 3.1433e-01,\n",
       "                      9.9045e-02, 1.2914e-01, 4.5410e-02, 6.8154e-02,\n",
       "                      5.3082e-01, 4.9543e-01, 4.1238e-02, 4.2012e-01,\n",
       "                      1.4271e-01, 3.1303e-01, 1.4734e-01, 3.4150e-01,\n",
       "                      1.1239e-01, 4.8653e-01, 7.8020e-02, 6.8225e-01,\n",
       "                      3.8020e-01, 4.1708e-01, 1.4414e-01, 2.1769e-01,\n",
       "                      4.1301e-01, 4.2307e-01, 1.2661e-01, 1.5997e-01,\n",
       "                      1.3271e-01, 5.0616e-02, 3.6107e-02, 2.7519e-02,\n",
       "                      3.6489e-01, 5.8515e-02, 5.1169e-01, 1.3434e-01,\n",
       "                      9.2413e-02, 1.3894e-01, 1.4029e-01, 2.8728e-01,\n",
       "                      7.2757e-01, 1.3781e-01, 2.2650e-01, 7.0864e-03,\n",
       "                      4.5216e-01, 1.2081e-01, 5.5454e-01, 1.6576e-01,\n",
       "                      9.3147e-02, 5.6364e-02, 4.4495e-01, 3.4007e-01,\n",
       "                      1.4453e-01, 1.0522e-01, 2.1546e-01, 5.4568e-01,\n",
       "                      1.1383e-01, 1.1415e-01, 1.7328e-01, 1.6626e-01,\n",
       "                      5.0640e-01, 3.9716e-01, 2.0355e-01, 1.0023e-01,\n",
       "                      2.4647e-02, 5.1966e-01, 2.0777e-01, 4.8693e-01,\n",
       "                      8.0533e-02, 1.4403e-01, 2.5042e-02, 5.6499e-01,\n",
       "                      1.0504e-01, 1.7985e-01, 1.2469e-01, 3.7980e-01,\n",
       "                      2.9787e-01, 7.6893e-02, 1.6971e-01, 1.7188e-01,\n",
       "                      2.9871e-01, 4.7774e-01, 9.8384e-02, 1.2795e-01,\n",
       "                      1.4820e-02, 3.8400e-02, 1.8365e-01, 1.7658e-01,\n",
       "                      2.3380e-01, 8.7784e-02, 2.9320e-02, 7.0149e-02,\n",
       "                      3.9157e-01, 1.2445e-01, 1.1522e-01, 2.2404e-01,\n",
       "                      3.5830e-01, 7.3788e-02, 2.3211e-01, 3.2627e-01,\n",
       "                      4.9369e-02, 1.1994e-01, 5.6320e-01, 2.5410e-01,\n",
       "                      5.1570e-02, 4.1780e-01, 1.4282e-01, 1.3876e-01,\n",
       "                      5.5131e-01, 8.2649e-02, 1.8249e-01, 1.1446e-01,\n",
       "                      1.5508e-01, 2.8819e-01, 4.3945e-01, 2.7515e-01,\n",
       "                      8.9956e-02, 4.2240e-01, 1.9058e-01, 2.0131e-01,\n",
       "                      3.1478e-01, 1.9241e-02, 8.7041e-02, 7.2134e-01,\n",
       "                      1.7343e-01, 2.7913e-01, 1.3616e-01, 1.7085e-01,\n",
       "                      5.1186e-01, 2.4192e-01, 1.0850e-01, 7.0185e-02,\n",
       "                      1.2253e-01, 9.0561e-03, 8.8243e-02, 5.2897e-03,\n",
       "                      6.8105e-02, 6.5281e-02, 5.4100e-01, 1.3007e-01,\n",
       "                      2.6060e-01, 3.9231e-01, 2.2155e-04, 5.2504e-01,\n",
       "                      3.8574e-01, 4.3523e-01, 2.0968e-01, 2.4594e-02,\n",
       "                      1.5066e-01, 4.6372e-01, 5.2277e-01, 2.9874e-02,\n",
       "                      5.1815e-02, 5.4020e-02, 7.1263e-02, 2.1796e-01,\n",
       "                      4.5586e-01, 2.9747e-01, 3.9378e-01, 2.8930e-01,\n",
       "                      3.2642e-01, 3.2487e-02, 9.9146e-02, 3.1984e-02,\n",
       "                      3.0934e-01, 5.9110e-01, 3.1925e-01, 1.4965e-01,\n",
       "                      3.1354e-01, 4.2731e-03, 1.8750e-01, 3.5900e-01,\n",
       "                      5.1458e-01, 5.2943e-02, 7.9039e-03, 1.4981e-01,\n",
       "                      4.6114e-02, 3.0759e-01, 5.1578e-01, 2.3200e-01,\n",
       "                      4.4332e-01, 2.3944e-01, 6.5596e-02, 1.4340e-01,\n",
       "                      1.7708e-01, 1.6549e-01, 5.9620e-01, 3.1874e-02,\n",
       "                      1.4911e-01, 1.1646e-01, 8.3340e-02, 3.9959e-01,\n",
       "                      5.5078e-01, 3.0014e-01, 1.5264e-02, 2.3141e-03,\n",
       "                      3.8772e-02, 1.7821e-01, 2.3968e-01, 1.6037e-01,\n",
       "                      1.2412e-01, 4.4715e-01, 2.3905e-02, 2.1334e-01,\n",
       "                      2.6776e-01, 2.0761e-02, 1.6338e-01, 3.3618e-01,\n",
       "                      6.1892e-01, 1.1428e-01, 2.5575e-01, 1.3798e-01,\n",
       "                      4.3495e-01, 1.0070e-01, 5.7128e-01, 1.5372e-01,\n",
       "                      6.7399e-02, 1.0929e-01, 1.4685e-01, 1.3318e-01,\n",
       "                      3.4109e-01, 1.2253e-01, 2.5213e-01, 4.6419e-01,\n",
       "                      3.8468e-01, 6.2928e-02, 1.4948e-01, 4.6930e-01,\n",
       "                      5.6550e-01, 2.2382e-01, 2.0766e-01, 4.7954e-01,\n",
       "                      1.0801e-01, 3.9226e-01, 3.2059e-01, 2.7604e-01,\n",
       "                      1.1528e-01, 8.1104e-02, 1.4631e-01, 3.6693e-01,\n",
       "                      2.3469e-01, 4.3884e-01, 3.5197e-01, 4.2413e-01,\n",
       "                      1.6786e-01, 1.3257e-01, 1.7175e-01, 3.2226e-02,\n",
       "                      5.2062e-01, 4.9496e-01, 3.4030e-01, 3.0856e-01,\n",
       "                      1.3992e-01, 1.0655e-01, 3.0277e-01, 3.5151e-01,\n",
       "                      9.9387e-02, 1.7605e-01, 1.8756e-01, 7.2674e-02,\n",
       "                      4.5600e-01, 4.5223e-01, 5.4075e-01, 3.9046e-01,\n",
       "                      3.7122e-01, 2.5523e-01, 1.3283e-01, 3.1408e-01,\n",
       "                      1.6410e-01, 3.0875e-01, 5.5912e-01, 2.5697e-01,\n",
       "                      2.1854e-01, 1.4360e-01, 7.2862e-02, 5.5702e-01,\n",
       "                      4.0269e-02, 1.0975e-01, 6.1083e-02, 7.4018e-02,\n",
       "                      2.8286e-02, 2.4810e-01, 4.6570e-01, 3.8805e-01,\n",
       "                      2.3156e-01, 4.1934e-01, 1.9273e-01, 9.0187e-02,\n",
       "                      1.2571e-01, 1.6840e-01, 7.7035e-02, 3.0582e-01,\n",
       "                      3.2137e-01, 2.6913e-01, 1.2959e-01, 1.0148e-01,\n",
       "                      1.8321e-01, 5.1596e-01, 1.5810e-01, 1.0493e-03,\n",
       "                      1.4517e-01, 1.7097e-02, 1.0066e-01, 6.5075e-02,\n",
       "                      1.8737e-01, 5.7776e-02, 2.2937e-01, 2.4102e-01,\n",
       "                      1.6675e-01, 2.8224e-01, 1.1691e-02, 8.1019e-02,\n",
       "                      7.0891e-02, 1.7853e-01, 4.6654e-02, 8.0092e-02,\n",
       "                      1.8386e-01, 1.4495e-01, 2.3524e-01, 2.7170e-01,\n",
       "                      7.8277e-02, 5.8737e-02, 3.2797e-02, 1.1474e-01,\n",
       "                      4.3792e-01, 4.3052e-01, 6.0764e-02, 5.3168e-02,\n",
       "                      1.3390e-01, 3.7483e-01, 1.9823e-01, 1.7578e-01,\n",
       "                      6.7577e-01, 1.1253e-01, 1.3687e-01, 3.9115e-01,\n",
       "                      4.8368e-01, 3.6260e-02, 1.9180e-01, 5.6709e-01,\n",
       "                      1.4300e-01, 3.4212e-02, 1.0259e-01, 1.6678e-01,\n",
       "                      5.6980e-02, 2.4424e-01, 2.2839e-01, 5.0682e-01,\n",
       "                      8.4394e-02, 1.0265e-01, 1.0926e-01, 2.1061e-01,\n",
       "                      5.2864e-02, 1.1416e-01, 3.3481e-01, 1.8446e-01,\n",
       "                      5.0502e-01, 3.5433e-01, 1.4211e-01, 5.6910e-03,\n",
       "                      5.5549e-01, 5.0727e-01, 1.1740e-01, 1.9103e-01,\n",
       "                      1.2084e-01, 4.2081e-02, 5.5378e-01, 2.8478e-01,\n",
       "                      8.5623e-02, 2.5110e-01, 1.3835e-01, 2.7161e-01,\n",
       "                      5.7083e-02, 5.0790e-02, 4.1169e-01, 7.5474e-01,\n",
       "                      4.2801e-01, 2.4692e-01, 3.9992e-01, 3.5321e-02,\n",
       "                      2.8323e-01, 1.4344e-01, 1.4920e-01, 8.2111e-02,\n",
       "                      1.5251e-01, 1.5940e-01, 2.0280e-01, 3.4484e-01,\n",
       "                      1.4662e-01, 3.0877e-01, 5.6605e-01, 3.2101e-01,\n",
       "                      3.0590e-02, 3.1238e-01, 5.1537e-02, 1.1838e-01,\n",
       "                      5.8916e-01, 1.9756e-01, 2.3342e-01, 2.8667e-01,\n",
       "                      1.5604e-01, 2.1663e-01, 5.3022e-02, 3.8530e-01,\n",
       "                      1.8670e-01, 9.5646e-02, 1.5902e-01, 2.6762e-01,\n",
       "                      2.6884e-01, 1.4936e-01, 8.8783e-02, 4.4187e-01,\n",
       "                      1.4817e-01, 2.8870e-01, 3.6679e-01, 9.2496e-02,\n",
       "                      6.0663e-01, 1.7511e-01, 5.2390e-01, 5.2030e-02,\n",
       "                      3.2272e-01, 1.6207e-01, 1.3508e-01, 6.9320e-03,\n",
       "                      7.1124e-02, 6.6895e-02, 6.7878e-02, 1.9237e-01,\n",
       "                      1.6426e-01, 3.3992e-01, 4.8942e-01, 4.5497e-01,\n",
       "                      1.3133e-01, 3.9293e-01, 2.1614e-01, 1.8161e-01,\n",
       "                      7.5614e-02, 4.8242e-01, 5.1040e-01, 7.6135e-01,\n",
       "                      2.9750e-02, 9.1280e-03, 8.4991e-02, 1.4919e-01,\n",
       "                      2.9241e-01, 3.5405e-01, 8.7982e-02, 4.2266e-02,\n",
       "                      7.4943e-02, 2.1405e-01, 2.1939e-01, 5.1464e-01,\n",
       "                      3.6182e-01, 3.8280e-01, 5.7101e-01, 3.7413e-01,\n",
       "                      3.7502e-01, 1.0108e-01, 3.0315e-01, 5.6176e-01,\n",
       "                      4.6801e-02, 2.0398e-01, 1.8555e-01, 1.0256e-01,\n",
       "                      1.2919e-01, 1.6607e-01, 6.5469e-02, 5.2886e-02,\n",
       "                      1.0836e-01, 1.5330e-01, 2.8108e-01, 2.7563e-01,\n",
       "                      3.3057e-01, 2.2736e-01, 4.2132e-01, 3.5100e-02,\n",
       "                      5.5955e-03, 2.1681e-01, 1.4030e-01, 1.8436e-01,\n",
       "                      3.9239e-01, 4.3209e-01, 1.0925e-01, 3.2261e-01,\n",
       "                      1.1509e-01, 3.7834e-01, 3.9896e-01, 4.6784e-01,\n",
       "                      4.9442e-02, 1.2577e-01, 1.5844e-01, 1.5678e-01,\n",
       "                      2.9184e-01, 3.0449e-01, 1.3827e-01, 2.9429e-01,\n",
       "                      3.2407e-01, 1.6188e-01, 2.8190e-01, 8.5010e-02,\n",
       "                      1.2962e-01, 5.8656e-01, 5.0267e-01, 3.5911e-01,\n",
       "                      5.4499e-02, 9.2269e-02, 3.6117e-01, 2.2298e-01,\n",
       "                      3.0913e-01, 7.2633e-02, 1.8594e-01, 1.5937e-02,\n",
       "                      5.3467e-01, 1.4085e-01, 1.5049e-01, 9.7213e-02,\n",
       "                      4.3992e-01, 3.7700e-01, 1.1148e-01, 2.2312e-02,\n",
       "                      1.1181e-01, 4.7968e-01, 2.1441e-01, 1.4569e-01,\n",
       "                      2.4512e-01, 1.8998e-01, 5.8036e-02, 4.8703e-01,\n",
       "                      4.0982e-01, 4.2575e-02, 2.0434e-02, 5.2229e-02,\n",
       "                      9.5785e-02, 9.9813e-02, 4.1528e-01, 1.1090e-01,\n",
       "                      3.5976e-01, 1.6081e-01, 1.0927e-01, 1.7029e-01,\n",
       "                      2.0391e-01, 1.7861e-01, 3.9411e-01, 3.6071e-01,\n",
       "                      3.1132e-01, 4.6320e-01, 3.3582e-01, 1.1063e-01,\n",
       "                      5.9487e-03, 1.8688e-01, 4.5544e-02, 1.7212e-01,\n",
       "                      1.6077e-01, 1.1755e-01, 5.5338e-01, 1.6856e-01,\n",
       "                      3.3485e-01, 2.9558e-01, 2.7053e-01, 2.3349e-01,\n",
       "                      2.3419e-01, 9.4214e-02, 4.0097e-02, 5.9944e-01,\n",
       "                      4.1706e-01, 6.4380e-01, 1.2384e-01, 1.8252e-01,\n",
       "                      9.1521e-02, 3.8744e-01, 2.4543e-01, 1.6593e-01,\n",
       "                      1.2731e-01, 2.8141e-02, 1.9035e-02, 3.5936e-01,\n",
       "                      4.4879e-01, 1.9850e-01, 4.4958e-01, 3.1280e-01,\n",
       "                      4.8285e-01, 5.3759e-02, 1.3630e-01, 1.8836e-01,\n",
       "                      1.5013e-01, 1.9314e-01, 6.5012e-02, 2.6904e-01,\n",
       "                      3.2655e-02, 3.1196e-02, 5.7262e-01, 5.2369e-01,\n",
       "                      3.9967e-01, 2.5869e-02, 1.6227e-01, 4.3028e-02,\n",
       "                      3.0723e-01, 4.9924e-01, 4.7678e-01, 1.1259e-01,\n",
       "                      1.4485e-01, 4.8759e-02]),\n",
       "       size=(30, 48, 48), nnz=630, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[    0,     1,     2,  ..., 37629, 37630, 37631],\n",
      "                       [    0,     0,     0,  ...,     2,     2,     2],\n",
      "                       [    0,     0,     0,  ...,   111,   111,   111],\n",
      "                       [    0,     1,     2,  ...,   109,   110,   111]]),\n",
      "       values=tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]),\n",
      "       size=(37632, 3, 112, 112), nnz=37632, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('app/src')\n",
    "sys.path.append('./src')\n",
    "from zono_sparse_gen import ZonoSparseGeneration\n",
    "test_input = torch.randn(3,112,112)\n",
    "_,zonotope_espilon_sparse_tensor = ZonoSparseGeneration(test_input,0.01).total_zono()\n",
    "\n",
    "print(zonotope_espilon_sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,  ..., 37631, 37631, 37631],\n",
      "        [    0,     1,     2,  ...,    27,    28,    29],\n",
      "        [    0,     0,     0,  ...,   109,   109,   109],\n",
      "        [    0,     0,     0,  ...,   109,   109,   109]])\n"
     ]
    }
   ],
   "source": [
    "result_2 =sparse_conv2d(zonotope_espilon_sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 37631, 37631, 37631],\n",
       "                       [    0,     1,     2,  ...,    27,    28,    29],\n",
       "                       [    0,     0,     0,  ...,   109,   109,   109],\n",
       "                       [    0,     0,     0,  ...,   109,   109,   109]]),\n",
       "       values=tensor([ 4.8458e-04, -1.7757e-03, -1.7573e-03,  ...,\n",
       "                      -3.7899e-04, -8.3448e-05, -6.4490e-04]),\n",
       "       size=(37632, 30, 110, 110), nnz=9801000, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
